{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "living-nurse",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Neural-Network-Foundations-with-TensorFlow-2.0\" data-toc-modified-id=\"Neural-Network-Foundations-with-TensorFlow-2.0-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Neural Network Foundations with TensorFlow 2.0</a></span></li><li><span><a href=\"#Perceptron\" data-toc-modified-id=\"Perceptron-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Perceptron</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-first-example-of-TensorFlow-2.0-code\" data-toc-modified-id=\"A-first-example-of-TensorFlow-2.0-code-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>A first example of TensorFlow 2.0 code</a></span></li></ul></li><li><span><a href=\"#multi-layer-perceptron-(MLP)\" data-toc-modified-id=\"multi-layer-perceptron-(MLP)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>multi-layer perceptron (MLP)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problems-in-training-the-perceptron-and-their-solutions\" data-toc-modified-id=\"Problems-in-training-the-perceptron-and-their-solutions-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Problems in training the perceptron and their solutions</a></span></li><li><span><a href=\"#Can-we-do-without-an-activation-function?\" data-toc-modified-id=\"Can-we-do-without-an-activation-function?-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Can we do without an activation function?</a></span></li><li><span><a href=\"#What-is-Activation-Function?\" data-toc-modified-id=\"What-is-Activation-Function?-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>What is Activation Function?</a></span></li><li><span><a href=\"#Why-we-use-Activation-functions-with-Neural-Networks?\" data-toc-modified-id=\"Why-we-use-Activation-functions-with-Neural-Networks?-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Why we use Activation functions with Neural Networks?</a></span></li></ul></li><li><span><a href=\"#Linear-or-Identity-Activation-Function\" data-toc-modified-id=\"Linear-or-Identity-Activation-Function-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Linear or Identity Activation Function</a></span></li><li><span><a href=\"#Non-linear-Activation-Function\" data-toc-modified-id=\"Non-linear-Activation-Function-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Non-linear Activation Function</a></span></li><li><span><a href=\"#Activation-Function-sigmoid\" data-toc-modified-id=\"Activation-Function-sigmoid-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Activation Function sigmoid</a></span></li><li><span><a href=\"#Activation-function-‚Äì-tanh\" data-toc-modified-id=\"Activation-function-‚Äì-tanh-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Activation function ‚Äì tanh</a></span></li><li><span><a href=\"#ReLU-(Rectified-Linear-Unit)-Activation-Function\" data-toc-modified-id=\"ReLU-(Rectified-Linear-Unit)-Activation-Function-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>ReLU (Rectified Linear Unit) Activation Function</a></span></li><li><span><a href=\"#Leaky-ReLU\" data-toc-modified-id=\"Leaky-ReLU-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Leaky ReLU</a></span><ul class=\"toc-item\"><li><span><a href=\"#Why-derivative/differentiation-is-used-?\" data-toc-modified-id=\"Why-derivative/differentiation-is-used-?-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Why derivative/differentiation is used ?</a></span></li><li><span><a href=\"#An-example-of-an-activation-function-applied-after-a-linear-function\" data-toc-modified-id=\"An-example-of-an-activation-function-applied-after-a-linear-function-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>An example of an activation function applied after a linear function</a></span></li><li><span><a href=\"#what-are-neural-networks-after-all?\" data-toc-modified-id=\"what-are-neural-networks-after-all?-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>what are neural networks after all?</a></span></li></ul></li><li><span><a href=\"#A-real-example-‚Äì-recognizing-handwritten-digits\" data-toc-modified-id=\"A-real-example-‚Äì-recognizing-handwritten-digits-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>A real example ‚Äì recognizing handwritten digits</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-hot-encoding\" data-toc-modified-id=\"One-hot-encoding-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>One-hot encoding</a></span></li><li><span><a href=\"#EPOCH\" data-toc-modified-id=\"EPOCH-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>EPOCH</a></span></li><li><span><a href=\"#BATCH_SIZE\" data-toc-modified-id=\"BATCH_SIZE-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>BATCH_SIZE</a></span></li><li><span><a href=\"#VALIDATION\" data-toc-modified-id=\"VALIDATION-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;</span>VALIDATION</a></span></li><li><span><a href=\"#Some-common-choices-for-objective-functions\" data-toc-modified-id=\"Some-common-choices-for-objective-functions-10.5\"><span class=\"toc-item-num\">10.5&nbsp;&nbsp;</span>Some common choices for objective functions</a></span></li><li><span><a href=\"#Some-common-choices-for-metrics-are:\" data-toc-modified-id=\"Some-common-choices-for-metrics-are:-10.6\"><span class=\"toc-item-num\">10.6&nbsp;&nbsp;</span>Some common choices for metrics are:</a></span></li></ul></li><li><span><a href=\"#Improving-the-simple-net-in-TensorFlow-2.0-with-hidden-layers\" data-toc-modified-id=\"Improving-the-simple-net-in-TensorFlow-2.0-with-hidden-layers-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Improving the simple net in TensorFlow 2.0 with hidden layers</a></span></li><li><span><a href=\"#Further-improving-the-simple-net-in-TensorFlow-with-Dropout\" data-toc-modified-id=\"Further-improving-the-simple-net-in-TensorFlow-with-Dropout-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Further improving the simple net in TensorFlow with Dropout</a></span></li><li><span><a href=\"#Testing-different-optimizers-in-TensorFlow-2.0\" data-toc-modified-id=\"Testing-different-optimizers-in-TensorFlow-2.0-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Testing different optimizers in TensorFlow 2.0</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gradient-Descent-(GD)\" data-toc-modified-id=\"Gradient-Descent-(GD)-13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;</span>Gradient Descent (GD)</a></span></li><li><span><a href=\"#learning-rate\" data-toc-modified-id=\"learning-rate-13.2\"><span class=\"toc-item-num\">13.2&nbsp;&nbsp;</span>learning rate</a></span></li></ul></li><li><span><a href=\"#Increasing-the-number-of-epochs\" data-toc-modified-id=\"Increasing-the-number-of-epochs-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Increasing the number of epochs</a></span></li><li><span><a href=\"#Controlling-the-optimizer-learning-rate\" data-toc-modified-id=\"Controlling-the-optimizer-learning-rate-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Controlling the optimizer learning rate</a></span></li><li><span><a href=\"#Increasing-the-number-of-internal-hidden-neurons\" data-toc-modified-id=\"Increasing-the-number-of-internal-hidden-neurons-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>Increasing the number of internal hidden neurons</a></span></li><li><span><a href=\"#Increasing-the-size-of-batch-computation\" data-toc-modified-id=\"Increasing-the-size-of-batch-computation-17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;</span>Increasing the size of batch computation</a></span></li><li><span><a href=\"#Regularization\" data-toc-modified-id=\"Regularization-18\"><span class=\"toc-item-num\">18&nbsp;&nbsp;</span>Regularization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Adopting-regularization-to-avoid-overfitting\" data-toc-modified-id=\"Adopting-regularization-to-avoid-overfitting-18.1\"><span class=\"toc-item-num\">18.1&nbsp;&nbsp;</span>Adopting regularization to avoid overfitting</a></span></li><li><span><a href=\"#increase-of-complexity-might-have-two-negative-consequences\" data-toc-modified-id=\"increase-of-complexity-might-have-two-negative-consequences-18.2\"><span class=\"toc-item-num\">18.2&nbsp;&nbsp;</span>increase of complexity might have two negative consequences</a></span></li><li><span><a href=\"#There-are-three-different-types-of-regularization-used-in-machine-learning:\" data-toc-modified-id=\"There-are-three-different-types-of-regularization-used-in-machine-learning:-18.3\"><span class=\"toc-item-num\">18.3&nbsp;&nbsp;</span>There are three different types of regularization used in machine learning:</a></span></li><li><span><a href=\"#L1-regularization-Example\" data-toc-modified-id=\"L1-regularization-Example-18.4\"><span class=\"toc-item-num\">18.4&nbsp;&nbsp;</span>L1 regularization Example</a></span></li><li><span><a href=\"#L2-regularization-Example\" data-toc-modified-id=\"L2-regularization-Example-18.5\"><span class=\"toc-item-num\">18.5&nbsp;&nbsp;</span>L2 regularization Example</a></span></li><li><span><a href=\"#Elastic-Net-example\" data-toc-modified-id=\"Elastic-Net-example-18.6\"><span class=\"toc-item-num\">18.6&nbsp;&nbsp;</span>Elastic Net example</a></span></li><li><span><a href=\"#Understanding-BatchNormalization\" data-toc-modified-id=\"Understanding-BatchNormalization-18.7\"><span class=\"toc-item-num\">18.7&nbsp;&nbsp;</span>Understanding BatchNormalization</a></span><ul class=\"toc-item\"><li><span><a href=\"#issue-1\" data-toc-modified-id=\"issue-1-18.7.1\"><span class=\"toc-item-num\">18.7.1&nbsp;&nbsp;</span>issue 1</a></span></li><li><span><a href=\"#issue-2\" data-toc-modified-id=\"issue-2-18.7.2\"><span class=\"toc-item-num\">18.7.2&nbsp;&nbsp;</span>issue 2</a></span></li><li><span><a href=\"#overall:\" data-toc-modified-id=\"overall:-18.7.3\"><span class=\"toc-item-num\">18.7.3&nbsp;&nbsp;</span>overall:</a></span></li></ul></li></ul></li><li><span><a href=\"#Sentiment-analysis\" data-toc-modified-id=\"Sentiment-analysis-19\"><span class=\"toc-item-num\">19&nbsp;&nbsp;</span>Sentiment analysis</a></span></li><li><span><a href=\"#Hyperparameter-tuning-and-AutoML\" data-toc-modified-id=\"Hyperparameter-tuning-and-AutoML-20\"><span class=\"toc-item-num\">20&nbsp;&nbsp;</span>Hyperparameter tuning and AutoML</a></span></li><li><span><a href=\"#Predicting-output\" data-toc-modified-id=\"Predicting-output-21\"><span class=\"toc-item-num\">21&nbsp;&nbsp;</span>Predicting output</a></span></li><li><span><a href=\"#A-practical-overview-of-backpropagation\" data-toc-modified-id=\"A-practical-overview-of-backpropagation-22\"><span class=\"toc-item-num\">22&nbsp;&nbsp;</span>A practical overview of backpropagation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-brick",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T05:39:30.421401Z",
     "start_time": "2021-08-09T05:39:30.405401Z"
    }
   },
   "source": [
    "# Neural Network Foundations with TensorFlow 2.0\n",
    "\n",
    "Why Tensorflow \n",
    "\n",
    "    ‚Ä¢ It works with all popular languages such as Python, C++, Java, R, and Go.\n",
    "    ‚Ä¢ Keras ‚Äì a high-level neural network API that has been integrated with TensorFlow (in 2.0, Keras became the standard API for interacting with TensorFlow). This API specifies how software components should interact.\n",
    "    ‚Ä¢ TensorFlow allows model deployment and ease of use in production.\n",
    "    ‚Ä¢ Support for eager computation (see Chapter 2, TensorFlow 1.x and 2.x) has been introduced in TensorFlow 2.0, in addition to graph computation based on static graphs.\n",
    "    ‚Ä¢ Most importantly, TensorFlow has very good community support.\n",
    "    \n",
    "What is Keras?\n",
    "\n",
    "Keras is a beautiful API for composing building blocks to create and train deep\n",
    "learning models. Keras is now part of TensorFlow. Another\n",
    "question is \"Should I use Keras or tf.keras?\" tf.keras is the implementation of\n",
    "Keras inside TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "facial-mills",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:08:47.595645Z",
     "start_time": "2021-08-11T02:08:47.558610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0.]\n",
      " [1. 0.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "W = tf.Variable(tf.ones(shape=(2,2)), name=\"W\")\n",
    "b = tf.Variable(tf.zeros(shape=(2)), name=\"b\")\n",
    "\n",
    "def model(x):\n",
    "    return W * x + b\n",
    "\n",
    "out_a = model([1,0])\n",
    "print(out_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-rehabilitation",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "The \"perceptron\" is a simple algorithm that, given an input vector x of m values (x1,x2,..., xm), often called input features or simply features, outputs either\n",
    "a 1 (\"yes\") or a 0 (\"no\"). \n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\n",
    "      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\n",
    "      \\end{array} \\right.\n",
    "\\tag{1}\\end{eqnarray}\n",
    "\n",
    "w is a vector of weights, wx is the dot product sigma ùë§ùëó * ùë•ùëó and b is bias. If you\n",
    "remember elementary geometry, wx + b defines a boundary hyperplane that changes\n",
    "position according to the values assigned to w and b. \n",
    "\n",
    "Note that a hyperplane is a subspace whose dimension is one less than that of its\n",
    "ambient space.\n",
    "\n",
    "## A first example of TensorFlow 2.0 code\n",
    "\n",
    "Each neuron can be initialized with specific weights via the kernel_initializer\n",
    "parameter. There are a few choices, the most common of which are listed as follows:\n",
    "1. random_uniform: Weights are initialized to uniformly random small values\n",
    "in the range -0.05 to 0.05.\n",
    "2. random_normal: Weights are initialized according to a Gaussian distribution,\n",
    "with zero mean and a small standard deviation of 0.05. For those of you who\n",
    "are not familiar with Gaussian distribution, think about a symmetric \"bell\n",
    "curve\" shape.\n",
    "3. zero: All weights are initialized to zero.\n",
    "\n",
    "\"perceptron\" was the name given to a model having one single linear\n",
    "layer, and as a consequence, if it has multiple layers, you would call it a multi-layer\n",
    "perceptron (MLP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "literary-wound",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:08:47.611640Z",
     "start_time": "2021-08-11T02:08:47.596639Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "NB_CLASSES = 10\n",
    "RESHAPED = 784\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "input_shape=(RESHAPED,), kernel_initializer='zeros', name='dense_layer', activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-respondent",
   "metadata": {},
   "source": [
    "#  multi-layer perceptron (MLP)\n",
    "Note that the input and the output layers are visible from outside, while all the other layers in the middle are hidden ‚Äì hence the name hidden layers.\n",
    "<img src=\"./i/A-hypothetical-example-of-Multilayer-Perceptron-Network.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-history",
   "metadata": {},
   "source": [
    "## Problems in training the perceptron and their solutions\n",
    "\n",
    "While the computer processes those images, we would like our neuron to adjust its weights\n",
    "and its bias so that we have fewer and fewer images wrongly recognized.\n",
    "\n",
    "we need a continuous function that allows us to compute the derivative.\n",
    "\n",
    "so, The activation functions help the network use the important information and suppress the irrelevant data points. not only 0 and 1 \n",
    "\n",
    "## Can we do without an activation function?\n",
    "We understand that using an activation function introduces an additional step at each layer during the forward propagation. Now the question is ‚Äì if the activation function increases the complexity so much, can we do without an activation function?\n",
    "\n",
    "Imagine a neural network without the activation functions. In that case, every neuron will only be performing a linear transformation on the inputs using the weights and biases. Although linear transformations make the neural network simpler, but this network would be less powerful and will not be able to learn the complex patterns from the data.\n",
    "\n",
    "## What is Activation Function?\n",
    "\n",
    "It‚Äôs just a thing function that you use to get the output of node. It is also known as Transfer Function.\n",
    "\n",
    "## Why we use Activation functions with Neural Networks?\n",
    "It is used to determine the output of neural network like yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. (depending upon the function).\n",
    "\n",
    "The Activation Functions can be basically divided into 2 types\n",
    "\n",
    "        Linear Activation Function\n",
    "        Non-linear Activation Functions\n",
    "\n",
    "# Linear or Identity Activation Function\n",
    "As you can see the function is a line or linear. Therefore, the output of the functions will not be confined between any range.\n",
    "\n",
    "\n",
    "<img src=\"./i/1_tldIgyDQWqm-sMwP7m3Bww.png\" />\n",
    "\n",
    "Equation : f(x) = x\n",
    "\n",
    "Range : (-infinity to infinity)\n",
    "\n",
    "It doesn‚Äôt help with the complexity or various parameters of usual data that is fed to the neural networks.\n",
    "\n",
    "# Non-linear Activation Function\n",
    "The Nonlinear Activation Functions are the most used activation functions. Nonlinearity helps to makes the graph look something like this\n",
    "\n",
    "\n",
    "# Activation Function sigmoid\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\sigma(x) \\equiv \\frac{1}{1+e^{-x}}.\n",
    "\\tag{3}\\end{eqnarray}\n",
    "\n",
    "the range (0, 1) when the input varies in the range (‚àí‚àû, ‚àû). Mathematically the function is continuous.\n",
    "\n",
    "The Sigmoid Function curve looks like a S-shape.\n",
    "\n",
    "<img src=\"./i/1_Xu7B5y9gp0iL5ooBj7LtWw.png\" />\n",
    "\n",
    "The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
    "\n",
    "The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.\n",
    "\n",
    "The function is monotonic but function‚Äôs derivative is not.\n",
    "\n",
    "The logistic sigmoid function can cause a neural network to get stuck at the training time.\n",
    "\n",
    "The softmax function is a more generalized logistic activation function which is used for multiclass classification.\n",
    "\n",
    "\n",
    "A neuron can use the sigmoid for computing the nonlinear function ùúé(ùëß = ùë§x + ùëè).\n",
    "\n",
    "Note that if z = wx + b is very large and positive, then ùëí^‚àíùëß ‚Üí 0 so ùúé(ùëß) ‚Üí 1, \n",
    "\n",
    "while if z = wx + b is very large and negative ùëí^‚àíùëß ‚Üí ‚àû so ùúé(ùëß) ‚Üí 0. \n",
    "\n",
    "In other words, a neuron\n",
    "with sigmoid activation has a behavior similar to the perceptron, but the changes are\n",
    "gradual and output values such as 0.5539 or 0.123191 are perfectly legitimate. In this\n",
    "sense, a sigmoid neuron can answer \"maybe.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "assisted-massage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:08:47.972454Z",
     "start_time": "2021-08-11T02:08:47.612609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# example plot for the sigmoid activation function\n",
    "from math import exp\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# sigmoid activation function\n",
    "def sigmoid(x):\n",
    "\treturn 1.0 / (1.0 + exp(-x))\n",
    " \n",
    "# define input data\n",
    "inputs = [x for x in range(-10, 10)]\n",
    "# calculate outputs\n",
    "outputs = [sigmoid(x) for x in inputs]\n",
    "# plot inputs vs outputs\n",
    "pyplot.plot(inputs, outputs)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-lightning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T18:35:05.956663Z",
     "start_time": "2021-08-09T18:35:05.947637Z"
    }
   },
   "source": [
    "# Activation function ‚Äì tanh\n",
    "tanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).\n",
    "\n",
    "<img src=\"./i/1_f9erByySVjTjohfFdNkJYQ.jpeg\" />\n",
    "\n",
    "Hyperbolic Tangent Function: \n",
    "\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\tanh(x) =  \\frac{e^{x} -e^{-x} }{e^{x} -e^{-x}}.\n",
    "\\tag{3}\\end{eqnarray}\n",
    "\n",
    "The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.\n",
    "\n",
    "The function is differentiable.\n",
    "\n",
    "The function is monotonic while its derivative is not monotonic.\n",
    "\n",
    "The tanh function is mainly used classification between two classes.\n",
    "\n",
    "Both tanh and logistic sigmoid activation functions are used in feed-forward nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "preceding-narrative",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:08:48.068484Z",
     "start_time": "2021-08-11T02:08:47.973491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example plot for the tanh activation function\n",
    "from math import exp\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# tanh activation function\n",
    "def tanh(x):\n",
    "\treturn (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "# define input data\n",
    "inputs = [x for x in range(-10, 10)]\n",
    "# calculate outputs\n",
    "outputs = [tanh(x) for x in inputs]\n",
    "# plot inputs vs outputs\n",
    "pyplot.plot(inputs, outputs)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-terminal",
   "metadata": {},
   "source": [
    "#  ReLU (Rectified Linear Unit) Activation Function\n",
    "The ReLU is the most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning.\n",
    "\n",
    "A ReLU is simply defined as f(x) =max(0, x) \n",
    "\n",
    "<img src=\"./i/1_XxxiA0jJvPrHEJHD4z893g.png\" />\n",
    "\n",
    "As you can see, the ReLU is half rectified (from bottom). f(z) is zero when z is less than zero and f(z) is equal to z when z is above or equal to zero.\n",
    "Range: [ 0 to infinity)\n",
    "\n",
    "The function and its derivative both are monotonic.\n",
    "\n",
    "But the issue is that all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly. \n",
    "\n",
    "That means any negative input given to the ReLU activation function turns the value into zero immediately in the graph, which in turns affects the resulting graph by not mapping the negative values appropriately.\n",
    "\n",
    "**very popular because it helps address some optimization problems observed with sigmoids.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-treat",
   "metadata": {},
   "source": [
    "# Leaky ReLU\n",
    "It is an attempt to solve the dying ReLU problem\n",
    "\n",
    "<img src=\"./i/1_A_Bzn0CjUgOXtPCJKnKLqA.jpeg\" />\n",
    "\n",
    "\n",
    "The leak helps to increase the range of the ReLU function. Usually, the value of a is 0.01 or so.\n",
    "When a is not 0.01 then it is called Randomized ReLU.\n",
    "\n",
    "Therefore the range of the Leaky ReLU is (-infinity to infinity).\n",
    "\n",
    "Both Leaky and Randomized ReLU functions are monotonic in nature. Also, their derivatives also monotonic in nature.\n",
    "\n",
    "\n",
    "<img src=\"./i/0_lo8wlkwReDcXkts0.png\" />\n",
    "     \n",
    "     \n",
    "## Why derivative/differentiation is used ?\n",
    "When updating the curve, to know in which direction and how much to change or update the curve depending upon the slope.That is why we use differentiation in almost every part of Machine Learning and Deep Learning.\n",
    "\n",
    "\n",
    "## An example of an activation function applied after a linear function\n",
    "<img src=\"./i/B14761_01_13.png\" />\n",
    "\n",
    "\n",
    "## what are neural networks after all?\n",
    "\n",
    "machine learning models are a way to compute a function that maps\n",
    "some inputs to their corresponding outputs. The function is nothing more than a\n",
    "number of addition and multiplication operations. However, when combined with\n",
    "a non-linear activation and stacked in multiple layers, these functions can learn\n",
    "almost anything.\n",
    "\n",
    "\n",
    "# A real example ‚Äì recognizing handwritten digits\n",
    "\n",
    "## One-hot encoding \n",
    "to transform categorical (nonnumerical) features into numerical variables. For instance, the categorical feature\n",
    "\"digit\" with value d in [0 ‚Äì 9] can be encoded into a binary vector with 10 positions,\n",
    "which always has 0 value except the d - th position where a 1 is present.\n",
    "\n",
    "## EPOCH \n",
    "\n",
    "defines how long the training should last.\n",
    "is the number of times the model is exposed to the training set. At\n",
    "each iteration the optimizer tries to adjust the weights so that the objective\n",
    "function is minimized.\n",
    "\n",
    "## BATCH_SIZE \n",
    "is the number of samples you feed in to your network at a time, \n",
    "## VALIDATION \n",
    "is the amount of data reserved for checking or proving the validity of the training process.\n",
    "\n",
    "The reason why we picked EPOCHS = 200, BATCH_SIZE = 128, VALIDATION_\n",
    "SPLIT=0.2, and N_HIDDEN = 128 will be clearer later in this chapter when we will\n",
    "explore different values and discuss hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "editorial-beginning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:08:48.260455Z",
     "start_time": "2021-08-11T02:08:48.069484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,), 10000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# network and training parameters\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of output \n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "N_HIDDEN = 128\n",
    "# Loading MNIST dataset.\n",
    "# verify\n",
    "# You can verify that the split between train and test is 60,000, and 0,000 respectively.\n",
    "# Labels have one-hot representation.is automatically applied\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train , Y_train) , (X_test , Y_test) = mnist.load_data()\n",
    "X_train.shape , Y_train.shape , X_test.shape , Y_test.shape ,  X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-ground",
   "metadata": {},
   "source": [
    "Typically, the values associated with each pixel are normalized in the range [0,1]\n",
    "(which means that the intensity of each pixel is divided by 255, the maximum\n",
    "intensity value). The output can be one of ten classes, with one class for each digit.\n",
    "\n",
    "The final layer is a single neuron with activation function \"softmax\", which is a\n",
    "generalization of the sigmoid function. \n",
    "\n",
    "\n",
    " As discussed earlier, a sigmoid function\n",
    "output is in the range (0, 1) when the input varies in the range (‚àí‚àû, ‚àû). \n",
    "\n",
    "Similarly,\n",
    "a softmax \"squashes\" a K-dimensional vector of arbitrary real values into a\n",
    "K-dimensional vector of real values in the range (0, 1), so that they all add up to 1.\n",
    "\n",
    "**In our case, it aggregates 10 answers provided by the previous layer with 10 neurons.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "indonesian-warning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:08:48.322491Z",
     "start_time": "2021-08-11T02:08:48.261480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train is 60000 rows of 28x28 values;\n",
    "# we --> reshape it to 60000 x 784.\n",
    "print(28*28)\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000 , RESHAPED).astype('float32')\n",
    "X_test = X_test.reshape( X_test.shape[0], RESHAPED).astype('float32')\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "rational-sullivan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:08:48.354484Z",
     "start_time": "2021-08-11T02:08:48.323474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Normalize inputs to be within in [0, 1].\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "coastal-credit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:08:48.370480Z",
     "start_time": "2021-08-11T02:08:48.356453Z"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot representation of the labels.\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "further-cherry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:08:48.386483Z",
     "start_time": "2021-08-11T02:08:48.371454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the model.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(NB_CLASSES , \n",
    "                             input_shape = (RESHAPED ,),\n",
    "                             name='dense_layer' , activation='softmax'\n",
    "                             \n",
    "                            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-conclusion",
   "metadata": {},
   "source": [
    "Once we define the model, we have to compile it so that it can be executed by\n",
    "TensorFlow 2.0. There are a few choices to be made during compilation. \n",
    "\n",
    "Firstly, we need to select an **optimizer**, which is the specific algorithm used to update\n",
    "weights while we train our model. \n",
    "\n",
    "Second, we need to select an objective function, which is used by the optimizer to navigate the space of weights (frequently, objective functions are called either **loss functions** or **cost functions** and the process of\n",
    "optimization is defined as a process of loss minimization). Typically, with neural networks, we seek to minimize the error. As such, the objective function is often referred to as a cost function or a loss function and the value calculated by the loss function is referred to as simply ‚Äúloss.‚Äù\n",
    "\n",
    " \n",
    "\n",
    "Third, we need to evaluate the trained model.\n",
    "\n",
    "\n",
    "\n",
    "## Some common choices for objective functions\n",
    "\n",
    "**1. MEAN SQUAR ROOT **:\n",
    "MSE, which defines the mean squared error between the predictions and the\n",
    "true values. Mathematically, if d is a vector of predictions and y is the vector\n",
    "of n observed values, then \n",
    "\n",
    "**Mean squared error & MSE** = $\\displaystyle\\frac{1}{n}\\sum_{t=1}^{n}(d-y)^2$   \n",
    "\n",
    "\n",
    "Note that this objective function\n",
    "is the average of all the mistakes made in each prediction. If a prediction is\n",
    "far off from the true value, then this distance is made more evident by the\n",
    "squaring operation. In addition, the square can add up the error regardless\n",
    "of whether a given value is positive or negative.\n",
    "\n",
    "\n",
    "**2.binary_crossentropy:** \n",
    "\n",
    "which defines the binary logarithmic loss. Suppose\n",
    "that our model predicts p while the target is c, then the binary cross-entropy\n",
    "is defined as ùêø(ùëù, ùëê) = ‚àíùëê ln(ùëù) ‚àí (1 ‚àí ùëê) ln(1 ‚àí ùëù). Note that this objective\n",
    "function is suitable for binary label prediction.\n",
    "\n",
    "**3.categorical_crossentropy:**\n",
    "\n",
    ", which defines the multiclass logarithmic\n",
    "loss. Categorical cross-entropy compares the distribution of the predictions\n",
    "with the true distribution, with the probability of the true class set to 1 and\n",
    "0 for the other classes. If the true class is c and the prediction is y, then the\n",
    "categorical cross-entropy is defined as: \n",
    "\n",
    "**Categorical_crossentropy** = $\\displaystyle\\sum_{i}C_i\\ln(p_i)$   \n",
    "\n",
    "\n",
    "\n",
    "One way to think about multi-class logarithm loss is to consider the true\n",
    "class represented as a one-hot encoded vector, and the closer the model's\n",
    "outputs are to that vector, the lower the loss. Note that this objective function\n",
    "is suitable for multi-class label predictions. It is also the default choice in\n",
    "association with softmax activation.\n",
    "\n",
    "## Some common choices for metrics are:\n",
    "\n",
    "**‚Ä¢ Accuracy**, which defines the proportion of correct predictions with respect to\n",
    "the targets\n",
    "\n",
    "**‚Ä¢ Precision**, which defines how many selected items are relevant for a multilabel classification\n",
    "\n",
    "**‚Ä¢ Recall**, which defines how many selected items are relevant for a multi-label\n",
    "classification\n",
    "\n",
    "**Metrics are similar to objective functions, with the only difference that they are\n",
    "not used for training a model, but only for evaluating the model.**\n",
    "\n",
    "As discussed, the loss function is used to optimize your network. This is the\n",
    "function minimized by the selected optimizer. Instead, a metric is used to judge the\n",
    "performance of your network. This is only for you to run an evaluation on and it\n",
    "should be separated from the optimization process. On some occasions, it would\n",
    "be ideal to directly optimize for a specific metric. However, some metrics are not\n",
    "differentiable with respect to their inputs, which precludes them from being used\n",
    "directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "virgin-witch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:08:48.417718Z",
     "start_time": "2021-08-11T02:08:48.387455Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compiling the model.\n",
    "model.compile(optimizer='SGD',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-israel",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD) is a particular kind of optimization algorithm used to reduce the mistakes made\n",
    "by neural networks after each training epoch.  Once the model is compiled, it\n",
    "can then be trained with the fit() method, which specifies a few parameters:\n",
    "    \n",
    "**‚Ä¢ epochs** is the number of times the model is exposed to the training set. At\n",
    "each iteration the optimizer tries to adjust the weights so that the objective\n",
    "function is minimized.\n",
    "\n",
    "**‚Ä¢ batch_size** is the number of training instances observed before the\n",
    "optimizer performs a weight update; there are usually many batches\n",
    "per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "guided-statistics",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:10:37.111423Z",
     "start_time": "2021-08-11T02:08:48.418401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.4019 - accuracy: 0.6574 - val_loss: 0.9032 - val_accuracy: 0.8266\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.7975 - accuracy: 0.8291 - val_loss: 0.6595 - val_accuracy: 0.8572\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.6455 - accuracy: 0.8505 - val_loss: 0.5629 - val_accuracy: 0.8708\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.5724 - accuracy: 0.8616 - val_loss: 0.5096 - val_accuracy: 0.8780\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.5280 - accuracy: 0.8684 - val_loss: 0.4752 - val_accuracy: 0.8834\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.4976 - accuracy: 0.8735 - val_loss: 0.4511 - val_accuracy: 0.8866\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.4751 - accuracy: 0.8782 - val_loss: 0.4328 - val_accuracy: 0.8896\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.4576 - accuracy: 0.8810 - val_loss: 0.4185 - val_accuracy: 0.8924\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.4436 - accuracy: 0.8832 - val_loss: 0.4069 - val_accuracy: 0.8946\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.4319 - accuracy: 0.8853 - val_loss: 0.3973 - val_accuracy: 0.8966\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.4221 - accuracy: 0.8876 - val_loss: 0.3893 - val_accuracy: 0.8982\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.4136 - accuracy: 0.8893 - val_loss: 0.3822 - val_accuracy: 0.8998\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.4063 - accuracy: 0.8905 - val_loss: 0.3761 - val_accuracy: 0.9009\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3998 - accuracy: 0.8917 - val_loss: 0.3707 - val_accuracy: 0.9018\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3940 - accuracy: 0.8931 - val_loss: 0.3660 - val_accuracy: 0.9031\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3888 - accuracy: 0.8940 - val_loss: 0.3617 - val_accuracy: 0.9031\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3840 - accuracy: 0.8954 - val_loss: 0.3578 - val_accuracy: 0.9048\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3797 - accuracy: 0.8961 - val_loss: 0.3542 - val_accuracy: 0.9053\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3758 - accuracy: 0.8968 - val_loss: 0.3510 - val_accuracy: 0.9062\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3722 - accuracy: 0.8976 - val_loss: 0.3480 - val_accuracy: 0.9058\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3688 - accuracy: 0.8986 - val_loss: 0.3453 - val_accuracy: 0.9072\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3657 - accuracy: 0.8993 - val_loss: 0.3426 - val_accuracy: 0.9082\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3628 - accuracy: 0.9000 - val_loss: 0.3403 - val_accuracy: 0.9083\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3601 - accuracy: 0.9003 - val_loss: 0.3382 - val_accuracy: 0.9095\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3576 - accuracy: 0.9012 - val_loss: 0.3362 - val_accuracy: 0.9099\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3552 - accuracy: 0.9017 - val_loss: 0.3341 - val_accuracy: 0.9097\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3529 - accuracy: 0.9022 - val_loss: 0.3324 - val_accuracy: 0.9106\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3507 - accuracy: 0.9026 - val_loss: 0.3307 - val_accuracy: 0.9099\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3487 - accuracy: 0.9035 - val_loss: 0.3289 - val_accuracy: 0.9106\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3468 - accuracy: 0.9037 - val_loss: 0.3275 - val_accuracy: 0.9113\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3450 - accuracy: 0.9040 - val_loss: 0.3260 - val_accuracy: 0.9118\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3433 - accuracy: 0.9047 - val_loss: 0.3245 - val_accuracy: 0.9116\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3416 - accuracy: 0.9054 - val_loss: 0.3233 - val_accuracy: 0.9122\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3400 - accuracy: 0.9057 - val_loss: 0.3219 - val_accuracy: 0.9126\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3385 - accuracy: 0.9060 - val_loss: 0.3207 - val_accuracy: 0.9130\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3370 - accuracy: 0.9066 - val_loss: 0.3196 - val_accuracy: 0.9127\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3357 - accuracy: 0.9063 - val_loss: 0.3184 - val_accuracy: 0.9130\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3343 - accuracy: 0.9074 - val_loss: 0.3175 - val_accuracy: 0.9137\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3330 - accuracy: 0.9076 - val_loss: 0.3164 - val_accuracy: 0.9134\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3318 - accuracy: 0.9081 - val_loss: 0.3156 - val_accuracy: 0.9137\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3306 - accuracy: 0.9082 - val_loss: 0.3144 - val_accuracy: 0.9143\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3294 - accuracy: 0.9083 - val_loss: 0.3136 - val_accuracy: 0.9140\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3283 - accuracy: 0.9085 - val_loss: 0.3127 - val_accuracy: 0.9145\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3272 - accuracy: 0.9092 - val_loss: 0.3119 - val_accuracy: 0.9144\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3262 - accuracy: 0.9093 - val_loss: 0.3110 - val_accuracy: 0.9143\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3252 - accuracy: 0.9095 - val_loss: 0.3102 - val_accuracy: 0.9147\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3243 - accuracy: 0.9099 - val_loss: 0.3094 - val_accuracy: 0.9145\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3233 - accuracy: 0.9103 - val_loss: 0.3088 - val_accuracy: 0.9143\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3224 - accuracy: 0.9103 - val_loss: 0.3081 - val_accuracy: 0.9151\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3215 - accuracy: 0.9107 - val_loss: 0.3072 - val_accuracy: 0.9152\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3206 - accuracy: 0.9110 - val_loss: 0.3067 - val_accuracy: 0.9151\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3198 - accuracy: 0.9111 - val_loss: 0.3061 - val_accuracy: 0.9154\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3190 - accuracy: 0.9114 - val_loss: 0.3054 - val_accuracy: 0.9157\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3182 - accuracy: 0.9116 - val_loss: 0.3047 - val_accuracy: 0.9156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3174 - accuracy: 0.9118 - val_loss: 0.3043 - val_accuracy: 0.9158\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3167 - accuracy: 0.9117 - val_loss: 0.3035 - val_accuracy: 0.9162\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3160 - accuracy: 0.9120 - val_loss: 0.3030 - val_accuracy: 0.9159\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3152 - accuracy: 0.9123 - val_loss: 0.3024 - val_accuracy: 0.9165\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3145 - accuracy: 0.9127 - val_loss: 0.3019 - val_accuracy: 0.9163\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3139 - accuracy: 0.9129 - val_loss: 0.3014 - val_accuracy: 0.9162\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3132 - accuracy: 0.9130 - val_loss: 0.3009 - val_accuracy: 0.9163\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3126 - accuracy: 0.9133 - val_loss: 0.3004 - val_accuracy: 0.9164\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3119 - accuracy: 0.9133 - val_loss: 0.3000 - val_accuracy: 0.9172\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3113 - accuracy: 0.9133 - val_loss: 0.2995 - val_accuracy: 0.9172\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3107 - accuracy: 0.9139 - val_loss: 0.2990 - val_accuracy: 0.9170\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3101 - accuracy: 0.9141 - val_loss: 0.2986 - val_accuracy: 0.9171\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3095 - accuracy: 0.9141 - val_loss: 0.2982 - val_accuracy: 0.9166\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3090 - accuracy: 0.9144 - val_loss: 0.2977 - val_accuracy: 0.9168\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3084 - accuracy: 0.9146 - val_loss: 0.2973 - val_accuracy: 0.9174\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3079 - accuracy: 0.9147 - val_loss: 0.2968 - val_accuracy: 0.9175\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3073 - accuracy: 0.9148 - val_loss: 0.2965 - val_accuracy: 0.9172\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3068 - accuracy: 0.9150 - val_loss: 0.2961 - val_accuracy: 0.9176\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3063 - accuracy: 0.9148 - val_loss: 0.2958 - val_accuracy: 0.9177\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3058 - accuracy: 0.9152 - val_loss: 0.2954 - val_accuracy: 0.9180\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3053 - accuracy: 0.9153 - val_loss: 0.2950 - val_accuracy: 0.9176\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3049 - accuracy: 0.9157 - val_loss: 0.2947 - val_accuracy: 0.9179\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3044 - accuracy: 0.9156 - val_loss: 0.2943 - val_accuracy: 0.9183\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3040 - accuracy: 0.9160 - val_loss: 0.2939 - val_accuracy: 0.9178\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3035 - accuracy: 0.9159 - val_loss: 0.2936 - val_accuracy: 0.9183\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3031 - accuracy: 0.9159 - val_loss: 0.2933 - val_accuracy: 0.9181\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3026 - accuracy: 0.9163 - val_loss: 0.2930 - val_accuracy: 0.9186\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3022 - accuracy: 0.9161 - val_loss: 0.2927 - val_accuracy: 0.9185\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3018 - accuracy: 0.9164 - val_loss: 0.2924 - val_accuracy: 0.9183\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3013 - accuracy: 0.9164 - val_loss: 0.2921 - val_accuracy: 0.9187\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3010 - accuracy: 0.9165 - val_loss: 0.2918 - val_accuracy: 0.9189\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3006 - accuracy: 0.9167 - val_loss: 0.2915 - val_accuracy: 0.9183\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.3002 - accuracy: 0.9165 - val_loss: 0.2912 - val_accuracy: 0.9188\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2997 - accuracy: 0.9168 - val_loss: 0.2910 - val_accuracy: 0.9182\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2994 - accuracy: 0.9169 - val_loss: 0.2906 - val_accuracy: 0.9190\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2990 - accuracy: 0.9169 - val_loss: 0.2904 - val_accuracy: 0.9191\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2986 - accuracy: 0.9167 - val_loss: 0.2901 - val_accuracy: 0.9194\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2983 - accuracy: 0.9171 - val_loss: 0.2898 - val_accuracy: 0.9195\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2979 - accuracy: 0.9172 - val_loss: 0.2896 - val_accuracy: 0.9198\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2976 - accuracy: 0.9173 - val_loss: 0.2893 - val_accuracy: 0.9200\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2972 - accuracy: 0.9177 - val_loss: 0.2892 - val_accuracy: 0.9193\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2969 - accuracy: 0.9176 - val_loss: 0.2889 - val_accuracy: 0.9200\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2966 - accuracy: 0.9175 - val_loss: 0.2887 - val_accuracy: 0.9193\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2962 - accuracy: 0.9176 - val_loss: 0.2884 - val_accuracy: 0.9205\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2959 - accuracy: 0.9178 - val_loss: 0.2883 - val_accuracy: 0.9199\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2956 - accuracy: 0.9177 - val_loss: 0.2880 - val_accuracy: 0.9204\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2952 - accuracy: 0.9179 - val_loss: 0.2877 - val_accuracy: 0.9203\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2950 - accuracy: 0.9182 - val_loss: 0.2875 - val_accuracy: 0.9204\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2946 - accuracy: 0.9182 - val_loss: 0.2873 - val_accuracy: 0.9205\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2943 - accuracy: 0.9180 - val_loss: 0.2870 - val_accuracy: 0.9201\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2940 - accuracy: 0.9182 - val_loss: 0.2869 - val_accuracy: 0.9206\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2937 - accuracy: 0.9182 - val_loss: 0.2867 - val_accuracy: 0.9210\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2935 - accuracy: 0.9187 - val_loss: 0.2865 - val_accuracy: 0.9209\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2932 - accuracy: 0.9181 - val_loss: 0.2863 - val_accuracy: 0.9206\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2929 - accuracy: 0.9183 - val_loss: 0.2860 - val_accuracy: 0.9209\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2926 - accuracy: 0.9187 - val_loss: 0.2859 - val_accuracy: 0.9206\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2923 - accuracy: 0.9186 - val_loss: 0.2858 - val_accuracy: 0.9208\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2920 - accuracy: 0.9183 - val_loss: 0.2855 - val_accuracy: 0.9214\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2918 - accuracy: 0.9189 - val_loss: 0.2853 - val_accuracy: 0.9206\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2915 - accuracy: 0.9189 - val_loss: 0.2851 - val_accuracy: 0.9212\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2913 - accuracy: 0.9191 - val_loss: 0.2849 - val_accuracy: 0.9213\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2910 - accuracy: 0.9189 - val_loss: 0.2848 - val_accuracy: 0.9213\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2907 - accuracy: 0.9191 - val_loss: 0.2846 - val_accuracy: 0.9212\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2905 - accuracy: 0.9191 - val_loss: 0.2844 - val_accuracy: 0.9216\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2902 - accuracy: 0.9189 - val_loss: 0.2844 - val_accuracy: 0.9218\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2900 - accuracy: 0.9194 - val_loss: 0.2841 - val_accuracy: 0.9212\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2897 - accuracy: 0.9195 - val_loss: 0.2840 - val_accuracy: 0.9215\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2895 - accuracy: 0.9193 - val_loss: 0.2838 - val_accuracy: 0.9215\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2893 - accuracy: 0.9195 - val_loss: 0.2836 - val_accuracy: 0.9217\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2890 - accuracy: 0.9195 - val_loss: 0.2835 - val_accuracy: 0.9215\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2887 - accuracy: 0.9197 - val_loss: 0.2832 - val_accuracy: 0.9212\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2885 - accuracy: 0.9194 - val_loss: 0.2831 - val_accuracy: 0.9221\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2883 - accuracy: 0.9196 - val_loss: 0.2830 - val_accuracy: 0.9218\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2881 - accuracy: 0.9196 - val_loss: 0.2828 - val_accuracy: 0.9216\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2879 - accuracy: 0.9199 - val_loss: 0.2827 - val_accuracy: 0.9218\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2876 - accuracy: 0.9198 - val_loss: 0.2826 - val_accuracy: 0.9214\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2874 - accuracy: 0.9199 - val_loss: 0.2825 - val_accuracy: 0.9215\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2872 - accuracy: 0.9200 - val_loss: 0.2823 - val_accuracy: 0.9216\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2870 - accuracy: 0.9201 - val_loss: 0.2821 - val_accuracy: 0.9215\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2868 - accuracy: 0.9201 - val_loss: 0.2820 - val_accuracy: 0.9218\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2866 - accuracy: 0.9203 - val_loss: 0.2819 - val_accuracy: 0.9217\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2864 - accuracy: 0.9205 - val_loss: 0.2817 - val_accuracy: 0.9220\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2862 - accuracy: 0.9204 - val_loss: 0.2817 - val_accuracy: 0.9218\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2859 - accuracy: 0.9206 - val_loss: 0.2815 - val_accuracy: 0.9217\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2857 - accuracy: 0.9203 - val_loss: 0.2814 - val_accuracy: 0.9218\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2856 - accuracy: 0.9205 - val_loss: 0.2811 - val_accuracy: 0.9220\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2854 - accuracy: 0.9207 - val_loss: 0.2811 - val_accuracy: 0.9221\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2852 - accuracy: 0.9207 - val_loss: 0.2809 - val_accuracy: 0.9222\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2850 - accuracy: 0.9209 - val_loss: 0.2808 - val_accuracy: 0.9221\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2848 - accuracy: 0.9206 - val_loss: 0.2807 - val_accuracy: 0.9221\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2846 - accuracy: 0.9206 - val_loss: 0.2807 - val_accuracy: 0.9220\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2844 - accuracy: 0.9207 - val_loss: 0.2805 - val_accuracy: 0.9221\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2842 - accuracy: 0.9210 - val_loss: 0.2804 - val_accuracy: 0.9218\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2840 - accuracy: 0.9208 - val_loss: 0.2802 - val_accuracy: 0.9223\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2838 - accuracy: 0.9210 - val_loss: 0.2801 - val_accuracy: 0.9224\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2837 - accuracy: 0.9211 - val_loss: 0.2799 - val_accuracy: 0.9222\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2835 - accuracy: 0.9210 - val_loss: 0.2799 - val_accuracy: 0.9227\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2833 - accuracy: 0.9212 - val_loss: 0.2797 - val_accuracy: 0.9225\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2831 - accuracy: 0.9213 - val_loss: 0.2797 - val_accuracy: 0.9222\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2830 - accuracy: 0.9211 - val_loss: 0.2795 - val_accuracy: 0.9225\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2828 - accuracy: 0.9212 - val_loss: 0.2795 - val_accuracy: 0.9221\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2826 - accuracy: 0.9214 - val_loss: 0.2793 - val_accuracy: 0.9226\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2825 - accuracy: 0.9212 - val_loss: 0.2792 - val_accuracy: 0.9227\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2823 - accuracy: 0.9214 - val_loss: 0.2791 - val_accuracy: 0.9225\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2821 - accuracy: 0.9212 - val_loss: 0.2789 - val_accuracy: 0.9226\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2819 - accuracy: 0.9218 - val_loss: 0.2789 - val_accuracy: 0.9227\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2818 - accuracy: 0.9214 - val_loss: 0.2788 - val_accuracy: 0.9227\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2816 - accuracy: 0.9216 - val_loss: 0.2787 - val_accuracy: 0.9229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2815 - accuracy: 0.9216 - val_loss: 0.2786 - val_accuracy: 0.9227\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2813 - accuracy: 0.9217 - val_loss: 0.2784 - val_accuracy: 0.9227\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2811 - accuracy: 0.9216 - val_loss: 0.2784 - val_accuracy: 0.9230\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2810 - accuracy: 0.9215 - val_loss: 0.2782 - val_accuracy: 0.9231\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2808 - accuracy: 0.9219 - val_loss: 0.2782 - val_accuracy: 0.9226\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2807 - accuracy: 0.9219 - val_loss: 0.2782 - val_accuracy: 0.9223\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2805 - accuracy: 0.9222 - val_loss: 0.2781 - val_accuracy: 0.9221\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2803 - accuracy: 0.9217 - val_loss: 0.2779 - val_accuracy: 0.9227\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2802 - accuracy: 0.9219 - val_loss: 0.2778 - val_accuracy: 0.9231\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2801 - accuracy: 0.9221 - val_loss: 0.2777 - val_accuracy: 0.9229\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2799 - accuracy: 0.9222 - val_loss: 0.2776 - val_accuracy: 0.9227\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2798 - accuracy: 0.9222 - val_loss: 0.2776 - val_accuracy: 0.9222\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2796 - accuracy: 0.9222 - val_loss: 0.2774 - val_accuracy: 0.9232\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2795 - accuracy: 0.9221 - val_loss: 0.2773 - val_accuracy: 0.9229\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2793 - accuracy: 0.9221 - val_loss: 0.2773 - val_accuracy: 0.9227\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2792 - accuracy: 0.9221 - val_loss: 0.2772 - val_accuracy: 0.9224\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2790 - accuracy: 0.9223 - val_loss: 0.2771 - val_accuracy: 0.9233\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2789 - accuracy: 0.9223 - val_loss: 0.2771 - val_accuracy: 0.9227\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2788 - accuracy: 0.9222 - val_loss: 0.2769 - val_accuracy: 0.9228\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2786 - accuracy: 0.9224 - val_loss: 0.2768 - val_accuracy: 0.9231\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2785 - accuracy: 0.9223 - val_loss: 0.2768 - val_accuracy: 0.9226\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2784 - accuracy: 0.9225 - val_loss: 0.2767 - val_accuracy: 0.9234\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2782 - accuracy: 0.9225 - val_loss: 0.2766 - val_accuracy: 0.9231\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2781 - accuracy: 0.9224 - val_loss: 0.2766 - val_accuracy: 0.9231\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2780 - accuracy: 0.9222 - val_loss: 0.2764 - val_accuracy: 0.9236\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2778 - accuracy: 0.9226 - val_loss: 0.2763 - val_accuracy: 0.9229\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2777 - accuracy: 0.9224 - val_loss: 0.2764 - val_accuracy: 0.9230\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2776 - accuracy: 0.9226 - val_loss: 0.2763 - val_accuracy: 0.9227\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2774 - accuracy: 0.9225 - val_loss: 0.2762 - val_accuracy: 0.9229\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2773 - accuracy: 0.9228 - val_loss: 0.2761 - val_accuracy: 0.9232\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2772 - accuracy: 0.9225 - val_loss: 0.2761 - val_accuracy: 0.9235\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2771 - accuracy: 0.9227 - val_loss: 0.2759 - val_accuracy: 0.9230\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2769 - accuracy: 0.9228 - val_loss: 0.2758 - val_accuracy: 0.9233\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2768 - accuracy: 0.9230 - val_loss: 0.2758 - val_accuracy: 0.9233\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2767 - accuracy: 0.9228 - val_loss: 0.2758 - val_accuracy: 0.9231\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2765 - accuracy: 0.9229 - val_loss: 0.2756 - val_accuracy: 0.9237\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2764 - accuracy: 0.9231 - val_loss: 0.2755 - val_accuracy: 0.9237\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2763 - accuracy: 0.9229 - val_loss: 0.2756 - val_accuracy: 0.9233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22d83f98a08>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model.\n",
    "model.fit(X_train, Y_train,\n",
    "            batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "            verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-coverage",
   "metadata": {},
   "source": [
    "Note that to_categorical(Y_train, NB_CLASSES) converts the array Y_train into\n",
    "a matrix with as many columns as there are classes. The number of rows stays the\n",
    "same. So, for instance if we have:\n",
    "    \n",
    "> labels\n",
    "\n",
    "array([0, 2, 1, 2, 0])\n",
    "\n",
    "then:\n",
    "    \n",
    "to_categorical(labels)\n",
    "\n",
    "array([[ 1., 0., 0.],\n",
    "\n",
    "       [ 0., 0., 1.],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "oriental-stage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:10:37.252395Z",
     "start_time": "2021-08-11T02:10:37.112394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, Y_test, batch_size=128 , verbose = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "human-rochester",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:10:37.268394Z",
     "start_time": "2021-08-11T02:10:37.253394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss, test acc: [0.27712858544588087, 0.9223]\n"
     ]
    }
   ],
   "source": [
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "minute-protein",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:10:37.284406Z",
     "start_time": "2021-08-11T02:10:37.269394Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 92.22999811172485  %\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy:', results[1] * 100 , \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-convergence",
   "metadata": {},
   "source": [
    "# Improving the simple net in TensorFlow 2.0 with hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "successful-nepal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:10:37.585394Z",
     "start_time": "2021-08-11T02:10:37.288412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# Network and training.\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# Loading MNIST dataset.\n",
    "# Labels have one-hot representation.\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize inputs to be within in [0, 1].\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "\n",
    "\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Labels have one-hot representation.\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "hungry-limit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:10:37.633394Z",
     "start_time": "2021-08-11T02:10:37.586394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN,input_shape=(RESHAPED,), name='dense_layer', activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2', activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
    "# Summary of the model.\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "turkish-watson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:11:10.220393Z",
     "start_time": "2021-08-11T02:10:37.634394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.4090 - accuracy: 0.6626 - val_loss: 0.7006 - val_accuracy: 0.8485\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5708 - accuracy: 0.8579 - val_loss: 0.4410 - val_accuracy: 0.8863\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4271 - accuracy: 0.8842 - val_loss: 0.3682 - val_accuracy: 0.9018\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3713 - accuracy: 0.8959 - val_loss: 0.3329 - val_accuracy: 0.9063\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3392 - accuracy: 0.9040 - val_loss: 0.3086 - val_accuracy: 0.9114\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3165 - accuracy: 0.9110 - val_loss: 0.2905 - val_accuracy: 0.9164\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.2991 - accuracy: 0.9151 - val_loss: 0.2774 - val_accuracy: 0.9208\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2845 - accuracy: 0.9194 - val_loss: 0.2658 - val_accuracy: 0.9236\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2720 - accuracy: 0.9231 - val_loss: 0.2549 - val_accuracy: 0.9252\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2605 - accuracy: 0.9264 - val_loss: 0.2467 - val_accuracy: 0.9298\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2508 - accuracy: 0.9284 - val_loss: 0.2375 - val_accuracy: 0.9318\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.2417 - accuracy: 0.9313 - val_loss: 0.2302 - val_accuracy: 0.9338\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.2336 - accuracy: 0.9338 - val_loss: 0.2233 - val_accuracy: 0.9377\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2257 - accuracy: 0.9358 - val_loss: 0.2171 - val_accuracy: 0.9380\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2187 - accuracy: 0.9379 - val_loss: 0.2112 - val_accuracy: 0.9411\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2117 - accuracy: 0.9398 - val_loss: 0.2058 - val_accuracy: 0.9418\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.2056 - accuracy: 0.9414 - val_loss: 0.2009 - val_accuracy: 0.9435\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1997 - accuracy: 0.9429 - val_loss: 0.1970 - val_accuracy: 0.9445\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1944 - accuracy: 0.9445 - val_loss: 0.1918 - val_accuracy: 0.9456\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1890 - accuracy: 0.9463 - val_loss: 0.1869 - val_accuracy: 0.9477\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1840 - accuracy: 0.9472 - val_loss: 0.1849 - val_accuracy: 0.9477\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1793 - accuracy: 0.9483 - val_loss: 0.1790 - val_accuracy: 0.9490\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1748 - accuracy: 0.9495 - val_loss: 0.1762 - val_accuracy: 0.9502\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1708 - accuracy: 0.9506 - val_loss: 0.1730 - val_accuracy: 0.9513\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1668 - accuracy: 0.9521 - val_loss: 0.1691 - val_accuracy: 0.9525\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1627 - accuracy: 0.9535 - val_loss: 0.1664 - val_accuracy: 0.9541\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1593 - accuracy: 0.9540 - val_loss: 0.1641 - val_accuracy: 0.9538\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1555 - accuracy: 0.9554 - val_loss: 0.1612 - val_accuracy: 0.9541\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1521 - accuracy: 0.9561 - val_loss: 0.1584 - val_accuracy: 0.9554\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1489 - accuracy: 0.9571 - val_loss: 0.1562 - val_accuracy: 0.9553\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1458 - accuracy: 0.9582 - val_loss: 0.1542 - val_accuracy: 0.9575\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1428 - accuracy: 0.9593 - val_loss: 0.1513 - val_accuracy: 0.9571\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1398 - accuracy: 0.9609 - val_loss: 0.1490 - val_accuracy: 0.9582\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1370 - accuracy: 0.9608 - val_loss: 0.1471 - val_accuracy: 0.9592\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1342 - accuracy: 0.9618 - val_loss: 0.1453 - val_accuracy: 0.9593\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1317 - accuracy: 0.9624 - val_loss: 0.1438 - val_accuracy: 0.9605\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1291 - accuracy: 0.9631 - val_loss: 0.1413 - val_accuracy: 0.9603\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1266 - accuracy: 0.9640 - val_loss: 0.1412 - val_accuracy: 0.9605\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1243 - accuracy: 0.9649 - val_loss: 0.1388 - val_accuracy: 0.9613\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1221 - accuracy: 0.9649 - val_loss: 0.1383 - val_accuracy: 0.9610\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1200 - accuracy: 0.9663 - val_loss: 0.1352 - val_accuracy: 0.9620\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1176 - accuracy: 0.9671 - val_loss: 0.1343 - val_accuracy: 0.9626\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1155 - accuracy: 0.9668 - val_loss: 0.1326 - val_accuracy: 0.9628\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1136 - accuracy: 0.9682 - val_loss: 0.1313 - val_accuracy: 0.9627\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1116 - accuracy: 0.9684 - val_loss: 0.1300 - val_accuracy: 0.9636\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1096 - accuracy: 0.9690 - val_loss: 0.1291 - val_accuracy: 0.9638\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1078 - accuracy: 0.9695 - val_loss: 0.1288 - val_accuracy: 0.9647\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1059 - accuracy: 0.9701 - val_loss: 0.1270 - val_accuracy: 0.9646\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1043 - accuracy: 0.9704 - val_loss: 0.1248 - val_accuracy: 0.9658\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1025 - accuracy: 0.9708 - val_loss: 0.1242 - val_accuracy: 0.9659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22d83fcc248>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Compiling the model.\n",
    "model.compile(optimizer='SGD',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Training the model.\n",
    "model.fit(X_train, Y_train,\n",
    "            batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "            verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "oriented-cattle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:11:10.595351Z",
     "start_time": "2021-08-11T02:11:10.221352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluating the model.\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test , verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "younger-explanation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:11:10.611351Z",
     "start_time": "2021-08-11T02:11:10.596351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9643\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-postcard",
   "metadata": {},
   "source": [
    "Note that improvement stops (or they become almost imperceptible) after a certain\n",
    "number of epochs. In machine learning, this is a phenomenon called convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-notification",
   "metadata": {},
   "source": [
    "# Further improving the simple net in TensorFlow with Dropout\n",
    "\n",
    "We decide to randomly drop ‚Äì with\n",
    "the DROPOUT probability ‚Äì some of the values propagated inside our internal dense\n",
    "network of hidden layers during training. In machine learning this is a well-known\n",
    "form of regularization. Surprisingly enough, this idea of randomly dropping a few\n",
    "values can improve our performance. The idea behind this improvement is that\n",
    "random dropout forces the network to learn redundant patterns that are useful\n",
    "for better generalization.\n",
    "\n",
    "\n",
    "Dropout is a regularization technique for neural network models proposed by Srivastava, et al. in their 2014 paper Dropout: A Simple Way to Prevent Neural Networks from Overfitting . Dropout is a technique where randomly selected neurons are ignored during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "frank-remark",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:11:10.674351Z",
     "start_time": "2021-08-11T02:11:10.612351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DROPOUT = 0.3\n",
    "\n",
    "# Building the model.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN, input_shape=(RESHAPED,), name='dense_layer', activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "\n",
    "model.add(keras.layers.Dense(N_HIDDEN,name='dense_layer_2', activation='relu')) \n",
    "\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "\n",
    "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
    "# Summary of the model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "indoor-revision",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:11:47.456544Z",
     "start_time": "2021-08-11T02:11:10.675324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 1.7007 - accuracy: 0.4522 - val_loss: 0.9157 - val_accuracy: 0.8092\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.9332 - accuracy: 0.7120 - val_loss: 0.5266 - val_accuracy: 0.8752\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.7026 - accuracy: 0.7834 - val_loss: 0.4219 - val_accuracy: 0.8910\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.6007 - accuracy: 0.8181 - val_loss: 0.3724 - val_accuracy: 0.8977\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.5352 - accuracy: 0.8393 - val_loss: 0.3409 - val_accuracy: 0.9040\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4964 - accuracy: 0.8530 - val_loss: 0.3168 - val_accuracy: 0.9110\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.4649 - accuracy: 0.8619 - val_loss: 0.2999 - val_accuracy: 0.9153\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.4416 - accuracy: 0.8689 - val_loss: 0.2848 - val_accuracy: 0.9193\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.4191 - accuracy: 0.8777 - val_loss: 0.2734 - val_accuracy: 0.9215\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.4006 - accuracy: 0.8829 - val_loss: 0.2620 - val_accuracy: 0.9243\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3877 - accuracy: 0.8871 - val_loss: 0.2520 - val_accuracy: 0.9274\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3718 - accuracy: 0.8923 - val_loss: 0.2429 - val_accuracy: 0.9303\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3620 - accuracy: 0.8942 - val_loss: 0.2352 - val_accuracy: 0.9317\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3458 - accuracy: 0.8977 - val_loss: 0.2283 - val_accuracy: 0.9340\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3345 - accuracy: 0.9015 - val_loss: 0.2215 - val_accuracy: 0.9357\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3272 - accuracy: 0.9043 - val_loss: 0.2160 - val_accuracy: 0.9368\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3197 - accuracy: 0.9066 - val_loss: 0.2094 - val_accuracy: 0.9393\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.3074 - accuracy: 0.9104 - val_loss: 0.2039 - val_accuracy: 0.9408\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3011 - accuracy: 0.9127 - val_loss: 0.1993 - val_accuracy: 0.9427\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.2965 - accuracy: 0.9131 - val_loss: 0.1957 - val_accuracy: 0.9440\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2848 - accuracy: 0.9156 - val_loss: 0.1909 - val_accuracy: 0.9449\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.2832 - accuracy: 0.9173 - val_loss: 0.1868 - val_accuracy: 0.9461\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2760 - accuracy: 0.9183 - val_loss: 0.1818 - val_accuracy: 0.9477\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.2723 - accuracy: 0.9208 - val_loss: 0.1786 - val_accuracy: 0.9487\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.2641 - accuracy: 0.9229 - val_loss: 0.1754 - val_accuracy: 0.9504\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2603 - accuracy: 0.9233 - val_loss: 0.1723 - val_accuracy: 0.9507\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2538 - accuracy: 0.9257 - val_loss: 0.1684 - val_accuracy: 0.9521\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.2505 - accuracy: 0.9270 - val_loss: 0.1663 - val_accuracy: 0.9531\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2446 - accuracy: 0.9288 - val_loss: 0.1637 - val_accuracy: 0.9535\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.2400 - accuracy: 0.9300 - val_loss: 0.1602 - val_accuracy: 0.9554\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.2361 - accuracy: 0.9308 - val_loss: 0.1579 - val_accuracy: 0.9552\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2362 - accuracy: 0.9305 - val_loss: 0.1557 - val_accuracy: 0.9559\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.2280 - accuracy: 0.9336 - val_loss: 0.1539 - val_accuracy: 0.9564\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.2250 - accuracy: 0.9337 - val_loss: 0.1507 - val_accuracy: 0.9573\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2207 - accuracy: 0.9354 - val_loss: 0.1491 - val_accuracy: 0.9578\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2189 - accuracy: 0.9366 - val_loss: 0.1469 - val_accuracy: 0.9582\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2136 - accuracy: 0.9371 - val_loss: 0.1453 - val_accuracy: 0.9592\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2164 - accuracy: 0.9373 - val_loss: 0.1442 - val_accuracy: 0.9602\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2124 - accuracy: 0.9382 - val_loss: 0.1427 - val_accuracy: 0.9598\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.2067 - accuracy: 0.9400 - val_loss: 0.1401 - val_accuracy: 0.9599\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2061 - accuracy: 0.9399 - val_loss: 0.1385 - val_accuracy: 0.9605\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.2019 - accuracy: 0.9401 - val_loss: 0.1371 - val_accuracy: 0.9603\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.1997 - accuracy: 0.9427 - val_loss: 0.1360 - val_accuracy: 0.9614\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.1949 - accuracy: 0.9435 - val_loss: 0.1341 - val_accuracy: 0.9617\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.1937 - accuracy: 0.9436 - val_loss: 0.1325 - val_accuracy: 0.9617\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1916 - accuracy: 0.9442 - val_loss: 0.1312 - val_accuracy: 0.9625\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.1894 - accuracy: 0.9451 - val_loss: 0.1303 - val_accuracy: 0.9625\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.1902 - accuracy: 0.9439 - val_loss: 0.1289 - val_accuracy: 0.9626\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1843 - accuracy: 0.9463 - val_loss: 0.1278 - val_accuracy: 0.9625\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1807 - accuracy: 0.9472 - val_loss: 0.1263 - val_accuracy: 0.9632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22bbf2e6f08>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Compiling the model.\n",
    "model.compile(optimizer='SGD',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Training the model.\n",
    "model.fit(X_train, Y_train,batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "competitive-alabama",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:11:47.822598Z",
     "start_time": "2021-08-11T02:11:47.457543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluating the model.\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test , verbose = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "miniature-cloud",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:11:47.838574Z",
     "start_time": "2021-08-11T02:11:47.832584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9632\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-manor",
   "metadata": {},
   "source": [
    "Note that it has been frequently observed that networks with random dropout in\n",
    "internal hidden layers can \"generalize\" better on unseen examples contained in test\n",
    "sets. \n",
    "\n",
    "Intuitively, we can consider this phenomenon as each neuron becoming more\n",
    "capable because it knows it cannot depend on its neighbors. Also, because it forces\n",
    "information to be stored in a redundant way. During testing there is no dropout,\n",
    "so we are now using all our highly tuned neurons. \n",
    "\n",
    "In short, it is generally a good\n",
    "approach to test how a net performs when a dropout function is adopted.\n",
    "\n",
    "Besides that, note that training accuracy should still be above test accuracy,\n",
    "otherwise, we might be not training for long enough. This is the case in our example\n",
    "and therefore we should increase the number of epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-sunday",
   "metadata": {},
   "source": [
    "# Testing different optimizers in TensorFlow 2.0\n",
    "\n",
    "\n",
    "## Gradient Descent (GD)\n",
    "\n",
    "<img src=\"./i/ball.png\" />\n",
    "\n",
    "The gradient descent can be seen as a hiker who needs to navigate down a steep\n",
    "slope and aims to enter a ditch. The slope represents the function C while the ditch\n",
    "represents the minimum Cmin. The hiker has a starting point w0. The hiker moves little\n",
    "by little; imagine that there is almost zero visibility, so the hiker cannot see where\n",
    "to go automatically, and they proceed in a zigzag. At each step r, the gradient is the\n",
    "direction of maximum increase.\n",
    "\n",
    "Mathematically this direction is the value of the partial derivative \n",
    "\n",
    "$\\displaystyle\\frac{ùúïc}{ùúïùë§}$\n",
    "\n",
    "opposite direction is   $\\displaystyle- \\frac{ùúïc}{ùúïùë§}(w_r)$\n",
    "\n",
    "\n",
    "evaluated at point wr, reached at step r. \n",
    "\n",
    "## learning rate\n",
    "\n",
    "At each step, the hiker can decide how big a stride to take before the next stop. This\n",
    "is the so-called \"learning rate\" $\\displaystyleùúÇ$ ‚â• 0 in gradient descent jargon . Note that if $\\displaystyleùúÇ$ is too\n",
    "small, then the hiker will move slowly. However, if $\\displaystyleùúÇ$ is too high, then the hiker will\n",
    "possibly miss the ditch by stepping over it.\n",
    "\n",
    "<img src=\"./i/0_lo8wlkwReDcXkts0.png\" />\n",
    "\n",
    "TensorFlow computes the derivative on our behalf so we don't need to worry about implementing or computing it.\n",
    "\n",
    "**TensorFlow implements a fast variant of gradient descent known as SGD and many\n",
    "more advanced optimization techniques such as RMSProp and Adam. RMSProp\n",
    "and Adam include the concept of momentum (a velocity component), in addition to\n",
    "the acceleration component that SGD has. This allows faster convergence at the cost\n",
    "of more computation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "strange-worcester",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:11:59.006355Z",
     "start_time": "2021-08-11T02:11:47.841576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 2s 33us/sample - loss: 0.4776 - accuracy: 0.8555 - val_loss: 0.1889 - val_accuracy: 0.9435\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2223 - accuracy: 0.9340 - val_loss: 0.1307 - val_accuracy: 0.9607\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1723 - accuracy: 0.9496 - val_loss: 0.1151 - val_accuracy: 0.9656\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.1500 - accuracy: 0.9560 - val_loss: 0.1067 - val_accuracy: 0.9690\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.1293 - accuracy: 0.9613 - val_loss: 0.0958 - val_accuracy: 0.9747\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1173 - accuracy: 0.9646 - val_loss: 0.0973 - val_accuracy: 0.9738\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.1113 - accuracy: 0.9668 - val_loss: 0.0985 - val_accuracy: 0.9732\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.1032 - accuracy: 0.9691 - val_loss: 0.0966 - val_accuracy: 0.9739\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0971 - accuracy: 0.9706 - val_loss: 0.0994 - val_accuracy: 0.9746\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0934 - accuracy: 0.9721 - val_loss: 0.0988 - val_accuracy: 0.9761\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import datetime, os\n",
    "# Network and training.\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10  # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# Loading MNIST dataset.\n",
    "# Labels have one-hot representation.\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize inputs within [0, 1].\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# One-hot representations for labels.\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "# Building the model.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(\n",
    "    keras.layers.Dense(N_HIDDEN,\n",
    "                       input_shape=(RESHAPED, ),\n",
    "                       name='dense_layer',\n",
    "                       activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2',\n",
    "                             activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(\n",
    "    keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
    "\n",
    "# Summary of the model.\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model.\n",
    "# model.compile(optimizer='SGD',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# Compiling the model.\n",
    "model.compile(optimizer='RMSProp',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "logdir = os.path.join(\"logs\",\n",
    "                      datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "# Training the model.\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# Evaluating the model.\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "australian-safety",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:11:59.022402Z",
     "start_time": "2021-08-11T02:11:59.010292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9767\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "organic-large",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:12:07.796608Z",
     "start_time": "2021-08-11T02:11:59.023431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 0.5199 - accuracy: 0.8430 - val_loss: 0.1839 - val_accuracy: 0.9465\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.2309 - accuracy: 0.9310 - val_loss: 0.1375 - val_accuracy: 0.9585\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1787 - accuracy: 0.9463 - val_loss: 0.1127 - val_accuracy: 0.9653\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.1503 - accuracy: 0.9554 - val_loss: 0.1026 - val_accuracy: 0.9684\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1312 - accuracy: 0.9607 - val_loss: 0.0949 - val_accuracy: 0.9706\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.1164 - accuracy: 0.9645 - val_loss: 0.0934 - val_accuracy: 0.9716\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1036 - accuracy: 0.9677 - val_loss: 0.0878 - val_accuracy: 0.9742\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0950 - accuracy: 0.9708 - val_loss: 0.0840 - val_accuracy: 0.9745\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0894 - accuracy: 0.9718 - val_loss: 0.0787 - val_accuracy: 0.9755\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0857 - accuracy: 0.9729 - val_loss: 0.0832 - val_accuracy: 0.9753\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import datetime, os\n",
    "# Network and training.\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10  # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# Loading MNIST dataset.\n",
    "# Labels have one-hot representation.\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize inputs within [0, 1].\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# One-hot representations for labels.\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "# Building the model.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(\n",
    "    keras.layers.Dense(N_HIDDEN,\n",
    "                       input_shape=(RESHAPED, ),\n",
    "                       name='dense_layer',\n",
    "                       activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2',\n",
    "                             activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(\n",
    "    keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
    "\n",
    "# Summary of the model.\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model.\n",
    "# model.compile(optimizer='SGD',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# Compiling the model.\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Training the model.\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# Evaluating the model.\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test , verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "handed-helen",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:12:08.158620Z",
     "start_time": "2021-08-11T02:12:07.797581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9745\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, Y_test , verbose=0)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-survivor",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "reduced-comparison",
   "metadata": {},
   "source": [
    "# Increasing the number of epochs\n",
    "Unfortunately, this choice increases our computation time tenfold,\n",
    "yet gives us no gain. The experiment is unsuccessful, but we have learned that if\n",
    "we spend more time learning, we will not necessarily improve the result. Learning\n",
    "is more about adopting smart techniques and not necessarily about the time spent\n",
    "in computations. \n",
    "\n",
    "# Controlling the optimizer learning rate\n",
    "\n",
    "the best value reached by our three\n",
    "experiments [lr=0.1, lr=0.01, lr=0.001] is 0.1, which is the default learning rate for  the optimizer. \n",
    "\n",
    "# Increasing the number of internal hidden neurons\n",
    "\n",
    "\n",
    "By increasing the complexity of the model, the runtime increases significantly because there are more and more parameters to optimize. However, the gains that we are getting by increasing the size of the network decrease more\n",
    "and more as the network grows.\n",
    "\n",
    "increasing the complexity of the model, the runtime increases\n",
    "significantly because there are more and more parameters to optimize. However,\n",
    "the gains that we are getting by increasing the size of the network decrease more\n",
    "and more as the network grows \n",
    "\n",
    "# Increasing the size of batch computation\n",
    "\n",
    "Gradient descent tries to minimize the cost function on all the examples provided in\n",
    "the training sets and, at the same time, for all the features provided in input. SGD is a\n",
    "much less expensive variant that considers only BATCH_SIZE examples. So, let us see\n",
    "how it behaves when we change this parameter. As you can see, the best accuracy\n",
    "value is reached for a BATCH_SIZE=64 in our four experiments.\n",
    "\n",
    "\n",
    "# Regularization\n",
    "\n",
    "## Adopting regularization to avoid overfitting\n",
    "Intuitively, a good machine learning model should achieve a low error rate on\n",
    "training data. Mathematically this is equivalent to minimizing the loss function\n",
    "on the training data given the model:\n",
    "\n",
    "$\\displaystyle min: {loss(Training Data | Model)} $\n",
    "\n",
    "## increase of complexity might have two negative consequences\n",
    "1. First, a complex model might require a significant amount of time to be executed. \n",
    "\n",
    "2. Second, a complex model might achieve very good performance on training data, but perform quite badly on validation data. \n",
    "\n",
    "This is because the model is able to contrive relationships between many parameters in the specific training context, but these relationships in fact do not exist within a more generalized context. Causing a model to lose its\n",
    "ability to generalize in this manner is termed \"overfitting.\" \n",
    "\n",
    "**Again, learning is more about generalization than memorization.**\n",
    "\n",
    "\n",
    "<img src=\"./i/bia_loss_complex.png\" /> \n",
    "As a rule of thumb, if during the training we see that the loss increases on validation,\n",
    "after an initial decrease, then we have a problem of model complexity, which overfits\n",
    "to the training data.\n",
    "\n",
    "a model is nothing more than a vector of weights. Each weight affects the output,\n",
    "except for those which are zero, or very close to it. Therefore, the complexity of\n",
    "a model can be conveniently represented as the number of non-zero weights. In\n",
    "other words, if we have two models M1 and M2 achieving pretty much the same\n",
    "performance in terms of loss function, then we should choose the simplest model,\n",
    "the one which has the minimum number of non-zero weights.\n",
    "\n",
    "We can use a hyperparameter ùúÜ>=0 for controlling the importance of having a simple\n",
    "model, as in this formula:\n",
    "\n",
    "$\\displaystyle min: {loss(Training Data|Model)} + ùúÜ * complexity(Model) $\n",
    "\n",
    "\n",
    "## There are three different types of regularization used in machine learning:\n",
    "1. L1 regularization (also known as LASSO): The complexity of the model is expressed as the sum of the absolute values of the weights.\n",
    "2.  L2 regularization (also known as Ridge): The complexity of the model is expressed as the sum of the squares of the weights\n",
    "3. Elastic regularization: The complexity of the model is captured by a combination of the preceding two techniques\n",
    "\n",
    "Note that playing with regularization can be a good way to increase the performance\n",
    "of a network, particularly when there is an evident situation of overfitting.\n",
    "\n",
    "REMEMBER:\n",
    "\n",
    "\n",
    "The Dense layer takes three regularizers, which all default to None. These are kernel_regularizer(applied to weights), bias_regularizer(applied to bias unit), and activity_regularizer(applied to layer activation).\n",
    "\n",
    "\n",
    "##  L1 regularization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "pending-europe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T03:00:02.612619Z",
     "start_time": "2021-08-11T02:59:43.793371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 125,898\n",
      "Trainable params: 125,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 2s 47us/sample - loss: 7.4340 - accuracy: 0.7546 - val_loss: 0.8603 - val_accuracy: 0.8882\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 2s 37us/sample - loss: 0.8880 - accuracy: 0.8621 - val_loss: 0.6843 - val_accuracy: 0.9158\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 2s 37us/sample - loss: 0.7316 - accuracy: 0.8935 - val_loss: 0.5836 - val_accuracy: 0.9349\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 2s 38us/sample - loss: 0.6503 - accuracy: 0.9076 - val_loss: 0.5022 - val_accuracy: 0.9436:\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 2s 37us/sample - loss: 0.5882 - accuracy: 0.9184 - val_loss: 0.4944 - val_accuracy: 0.9459\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 2s 37us/sample - loss: 0.5476 - accuracy: 0.9275 - val_loss: 0.4424 - val_accuracy: 0.9523\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 2s 34us/sample - loss: 0.5168 - accuracy: 0.9327 - val_loss: 0.4618 - val_accuracy: 0.9514\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 2s 38us/sample - loss: 0.4938 - accuracy: 0.9388 - val_loss: 0.4437 - val_accuracy: 0.9538\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 2s 36us/sample - loss: 0.4725 - accuracy: 0.9418 - val_loss: 0.4172 - val_accuracy: 0.9576\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 2s 35us/sample - loss: 0.4548 - accuracy: 0.9456 - val_loss: 0.3784 - val_accuracy: 0.9622\n",
      "Test accuracy: 0.9602\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "# from tensorflow.keras.regularizers import l2, activity_l2\n",
    "\n",
    "import datetime, os\n",
    "# Network and training.\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 68\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10  # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# Loading MNIST dataset.\n",
    "# Labels have one-hot representation.\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize inputs within [0, 1].\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# One-hot representations for labels.\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "# Building the model.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                       input_shape=(RESHAPED, ),\n",
    "                       name='dense_layer',\n",
    "                       activation='relu' ,  \n",
    "#                              kernel_regularizer=keras.regularizers.l1(l=0.1)\n",
    "                            ))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2',\n",
    "                             activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "\n",
    "## apply regularization\n",
    "model.add(keras.layers.Dense(64, input_dim=64,   kernel_regularizer=keras.regularizers.l1(l=0.1) ,activation='relu'  ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
    "\n",
    "# Summary of the model.\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model.\n",
    "# model.compile(optimizer='SGD',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# Compiling the model.\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Training the model.\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test , verbose=0)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-wayne",
   "metadata": {},
   "source": [
    "##  L2 regularization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "tribal-square",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:58:21.578176Z",
     "start_time": "2021-08-11T02:58:02.043402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 125,898\n",
      "Trainable params: 125,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 3s 53us/sample - loss: 2.5664 - accuracy: 0.7582 - val_loss: 0.8382 - val_accuracy: 0.8633\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 2s 36us/sample - loss: 0.8780 - accuracy: 0.8392 - val_loss: 0.6936 - val_accuracy: 0.8982\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 2s 37us/sample - loss: 0.8121 - accuracy: 0.8539 - val_loss: 0.6711 - val_accuracy: 0.9005\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 2s 37us/sample - loss: 0.7696 - accuracy: 0.8638 - val_loss: 0.6282 - val_accuracy: 0.9106\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 2s 38us/sample - loss: 0.7445 - accuracy: 0.8702 - val_loss: 0.6015 - val_accuracy: 0.9114\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 2s 36us/sample - loss: 0.7327 - accuracy: 0.8722 - val_loss: 0.5979 - val_accuracy: 0.9197\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 2s 38us/sample - loss: 0.7193 - accuracy: 0.8743 - val_loss: 0.5877 - val_accuracy: 0.9209\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 2s 40us/sample - loss: 0.7072 - accuracy: 0.8781 - val_loss: 0.6214 - val_accuracy: 0.9013\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 2s 37us/sample - loss: 0.6915 - accuracy: 0.8807 - val_loss: 0.5491 - val_accuracy: 0.9260\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 2s 38us/sample - loss: 0.6889 - accuracy: 0.8798 - val_loss: 0.5525 - val_accuracy: 0.9233\n",
      "Test accuracy: 0.9204\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import datetime, os\n",
    "# Network and training.\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 68\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10  # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# Loading MNIST dataset.\n",
    "# Labels have one-hot representation.\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize inputs within [0, 1].\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# One-hot representations for labels.\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "# Building the model.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                       input_shape=(RESHAPED, ),\n",
    "                       name='dense_layer',\n",
    "                       activation='relu' ,  kernel_regularizer=keras.regularizers.l2(l=0.1)))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2',\n",
    "                             activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "\n",
    "## apply regularization\n",
    "model.add(keras.layers.Dense(64, input_dim=64,   kernel_regularizer=keras.regularizers.l2(l=0.1) ,activation='relu'  ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
    "\n",
    "# Summary of the model.\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model.\n",
    "# model.compile(optimizer='SGD',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# Compiling the model.\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Training the model.\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test , verbose=0)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-latitude",
   "metadata": {},
   "source": [
    "## Elastic Net example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "adjustable-entrepreneur",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T03:01:55.399748Z",
     "start_time": "2021-08-11T03:01:36.201432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 125,898\n",
      "Trainable params: 125,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 2s 50us/sample - loss: 7.5872 - accuracy: 0.7460 - val_loss: 0.8994 - val_accuracy: 0.8907\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 2s 37us/sample - loss: 0.9143 - accuracy: 0.8595 - val_loss: 0.7146 - val_accuracy: 0.9100\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 2s 38us/sample - loss: 0.7566 - accuracy: 0.8877 - val_loss: 0.6179 - val_accuracy: 0.9266\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 2s 38us/sample - loss: 0.6769 - accuracy: 0.9038 - val_loss: 0.5597 - val_accuracy: 0.9350\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 2s 36us/sample - loss: 0.6092 - accuracy: 0.9153 - val_loss: 0.4802 - val_accuracy: 0.9461\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 2s 38us/sample - loss: 0.5711 - accuracy: 0.9233 - val_loss: 0.4895 - val_accuracy: 0.9474\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 2s 36us/sample - loss: 0.5387 - accuracy: 0.9317 - val_loss: 0.4591 - val_accuracy: 0.9515\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 2s 37us/sample - loss: 0.5214 - accuracy: 0.9336 - val_loss: 0.4679 - val_accuracy: 0.9513\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 2s 37us/sample - loss: 0.4996 - accuracy: 0.9371 - val_loss: 0.4329 - val_accuracy: 0.9595\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 2s 36us/sample - loss: 0.4797 - accuracy: 0.9437 - val_loss: 0.4053 - val_accuracy: 0.9592\n",
      "Test accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import datetime, os\n",
    "# Network and training.\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 68\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10  # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# Loading MNIST dataset.\n",
    "# Labels have one-hot representation.\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize inputs within [0, 1].\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# One-hot representations for labels.\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "# Building the model.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                       input_shape=(RESHAPED, ),\n",
    "                       name='dense_layer',\n",
    "                       activation='relu' ,  \n",
    "#                              kernel_regularizer=keras.regularizers.l1_l2(l1=0.1, l2=0.01)\n",
    "                            ))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2',\n",
    "                             activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "\n",
    "## apply regularization\n",
    "model.add(keras.layers.Dense(64, input_dim=64 ,activation='relu' ,\n",
    "                            kernel_regularizer=keras.regularizers.l1_l2(l1=0.1, l2=0.01)\n",
    "                            ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
    "\n",
    "# Summary of the model.\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model.\n",
    "# model.compile(optimizer='SGD',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# Compiling the model.\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Training the model.\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test , verbose=0)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-shore",
   "metadata": {},
   "source": [
    "## Understanding BatchNormalization\n",
    "BatchNormalization is another form of regularization.\n",
    "\n",
    "\n",
    "issue solve with BatchNormalization\n",
    "### issue 1\n",
    "each layer must continuously\n",
    "re-adjust its weights to the different distribution for every batch. This may slow\n",
    "down the model's training greatly. The key idea is to make layer inputs more similar\n",
    "in distribution, batch after batch and epoch after epoch.\n",
    "### issue 2\n",
    "\n",
    "Another issue is that the sigmoid activation function works very well close to\n",
    "zero, but tends to \"get stuck\" when values get sufficiently far away from zero. If,\n",
    "occasionally, neuron outputs fluctuate far away from the sigmoid zero, then said\n",
    "neuron becomes unable to update its own weights.\n",
    "\n",
    "### overall:\n",
    "\n",
    "BatchNormalization has been proven as a very\n",
    "effective way to increase both the speed of training and accuracy, because it helps\n",
    "to prevent activations becoming either too small and vanishing or too big and\n",
    "exploding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "polar-medication",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T03:17:12.430587Z",
     "start_time": "2021-08-11T03:16:56.497587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 2s 40us/sample - loss: 0.3498 - accuracy: 0.8967 - val_loss: 0.1492 - val_accuracy: 0.9592\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 1s 31us/sample - loss: 0.1432 - accuracy: 0.9569 - val_loss: 0.1167 - val_accuracy: 0.9654\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 1s 30us/sample - loss: 0.0993 - accuracy: 0.9698 - val_loss: 0.0977 - val_accuracy: 0.9692\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 1s 30us/sample - loss: 0.0777 - accuracy: 0.9763 - val_loss: 0.0863 - val_accuracy: 0.9730\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 1s 29us/sample - loss: 0.0601 - accuracy: 0.9816 - val_loss: 0.0931 - val_accuracy: 0.9724\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 1s 31us/sample - loss: 0.0501 - accuracy: 0.9845 - val_loss: 0.0888 - val_accuracy: 0.9752\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 2s 31us/sample - loss: 0.0417 - accuracy: 0.9863 - val_loss: 0.0867 - val_accuracy: 0.9746\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 1s 31us/sample - loss: 0.0359 - accuracy: 0.9884 - val_loss: 0.0839 - val_accuracy: 0.9778\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 2s 31us/sample - loss: 0.0302 - accuracy: 0.9898 - val_loss: 0.0935 - val_accuracy: 0.9771\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 1s 31us/sample - loss: 0.0270 - accuracy: 0.9909 - val_loss: 0.0894 - val_accuracy: 0.9764\n",
      "Test accuracy: 0.9788\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "# from tensorflow.keras.regularizers import l2, activity_l2\n",
    "\n",
    "import datetime, os\n",
    "# Network and training.\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 68\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10  # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# Loading MNIST dataset.\n",
    "# Labels have one-hot representation.\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize inputs within [0, 1].\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# One-hot representations for labels.\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Building the model.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                       input_shape=(RESHAPED, ),\n",
    "                       name='dense_layer',\n",
    "                       activation='relu')),\n",
    "\n",
    "BatchNormalization(),\n",
    "model.add(keras.layers.Dense(N_HIDDEN, name='dense_layer_2',\n",
    "                             activation='relu') ,)\n",
    "BatchNormalization(),\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "BatchNormalization(),\n",
    "\n",
    "\n",
    "\n",
    "model.add(keras.layers.Dense(NB_CLASSES, name='dense_layer_3', activation='softmax'))\n",
    "\n",
    "# Summary of the model.\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model.\n",
    "# model.compile(optimizer='SGD',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# Compiling the model.\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Training the model.\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test , verbose=0)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-mauritius",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "The IMDb dataset contains the text of 50,000\n",
    "movie reviews from the Internet Movie Database. Each review is either positive or\n",
    "negative (for example, thumbs up or thumbs down). The dataset is split into 25,000\n",
    "reviews for training and 25,000 reviews for testing. Our goal is to build a classifier\n",
    "that is able to predict the binary judgment given the text. We can easily load IMDb\n",
    "via tf.keras and the sequences of words in the reviews have been converted to\n",
    "sequences of integers, where each integer represents a specific word in a dictionary.\n",
    "We also have a convenient way of padding sentences to max_len, so that we can\n",
    "use all sentences, whether short or long, as inputs to a neural network with an input\n",
    "vector of fixed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "charged-table",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T03:27:57.049668Z",
     "start_time": "2021-08-11T03:27:57.040668Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "stunning-audit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T03:48:12.755970Z",
     "start_time": "2021-08-11T03:46:53.706306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 200, 256)          2560000   \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 200, 256)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,593,025\n",
      "Trainable params: 2,593,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 4s 168us/sample - loss: 0.6729 - accuracy: 0.6299 - val_loss: 0.6314 - val_accuracy: 0.7592\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 4s 149us/sample - loss: 0.4576 - accuracy: 0.8373 - val_loss: 0.3683 - val_accuracy: 0.8526\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 4s 146us/sample - loss: 0.2807 - accuracy: 0.8862 - val_loss: 0.3065 - val_accuracy: 0.8730\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 4s 146us/sample - loss: 0.2179 - accuracy: 0.9168 - val_loss: 0.2935 - val_accuracy: 0.8774\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 4s 146us/sample - loss: 0.1717 - accuracy: 0.9390 - val_loss: 0.2901 - val_accuracy: 0.8767\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 4s 148us/sample - loss: 0.1345 - accuracy: 0.9541 - val_loss: 0.3008 - val_accuracy: 0.8715\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 4s 148us/sample - loss: 0.1013 - accuracy: 0.9689 - val_loss: 0.3078 - val_accuracy: 0.8706\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 4s 150us/sample - loss: 0.0791 - accuracy: 0.9768 - val_loss: 0.3206 - val_accuracy: 0.8672\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 4s 149us/sample - loss: 0.0590 - accuracy: 0.9841 - val_loss: 0.3387 - val_accuracy: 0.8615\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 4s 147us/sample - loss: 0.0444 - accuracy: 0.9892 - val_loss: 0.3556 - val_accuracy: 0.8604\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 4s 149us/sample - loss: 0.0322 - accuracy: 0.9933 - val_loss: 0.3734 - val_accuracy: 0.8587\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 4s 149us/sample - loss: 0.0252 - accuracy: 0.9944 - val_loss: 0.4064 - val_accuracy: 0.8511\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 4s 147us/sample - loss: 0.0194 - accuracy: 0.9962 - val_loss: 0.4090 - val_accuracy: 0.8548\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 4s 153us/sample - loss: 0.0151 - accuracy: 0.9974 - val_loss: 0.4252 - val_accuracy: 0.8516\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 4s 146us/sample - loss: 0.0124 - accuracy: 0.9980 - val_loss: 0.4391 - val_accuracy: 0.8520\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 4s 146us/sample - loss: 0.0098 - accuracy: 0.9987 - val_loss: 0.4603 - val_accuracy: 0.8496\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 4s 148us/sample - loss: 0.0089 - accuracy: 0.9988 - val_loss: 0.4674 - val_accuracy: 0.8516\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 4s 147us/sample - loss: 0.0071 - accuracy: 0.9989 - val_loss: 0.4778 - val_accuracy: 0.8517\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 4s 148us/sample - loss: 0.0068 - accuracy: 0.9990 - val_loss: 0.4934 - val_accuracy: 0.8499\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 4s 147us/sample - loss: 0.0059 - accuracy: 0.9989 - val_loss: 0.5082 - val_accuracy: 0.8484\n",
      "\n",
      "Test score: 0.50817123234272\n",
      "Test accuracy: 0.8484\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, preprocessing\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "max_len = 200\n",
    "n_words = 10000\n",
    "dim_embedding = 256\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 500\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # Load data.\n",
    "    (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_words)\n",
    "    # Pad sequences with max_len.\n",
    "    X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "    X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    # Input: - eEmbedding Layer.\n",
    "    # The model will take as input an integer matrix of size (batch,\n",
    "    # input_length).\n",
    "    # The model will output dimension (input_length, dim_embedding).\n",
    "    # The largest integer in the input should be no larger\n",
    "    # than n_words (vocabulary size).\n",
    "    model.add(layers.Embedding(n_words,dim_embedding, input_length=max_len))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    # Takes the maximum value of either feature vector from each of\n",
    "    # the n_words features.\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
    "\n",
    "score = model.fit(X_train, y_train,epochs = EPOCHS,\n",
    "                batch_size = BATCH_SIZE,\n",
    "                validation_data = (X_test, y_test)\n",
    ")\n",
    "\n",
    "score = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE ,verbose=0)\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-sampling",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning and AutoML\n",
    "\n",
    "Hyperparameter tuning is the process of finding the optimal combination of those\n",
    "hyperparameters that minimize cost functions. The key idea is that if we have n\n",
    "hyperparameters, then we can imagine that they define a space with n dimensions\n",
    "and the goal is to find the point in this space that corresponds to an optimal value\n",
    "for the cost function. One way to achieve this goal is to create a grid in this space\n",
    "and systematically check the value assumed by the cost function for each grid\n",
    "vertex. In other words, the hyperparameters are divided into buckets and different\n",
    "combinations of values are checked via a brute force approach.\n",
    "\n",
    "# Predicting output\n",
    "\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# A practical overview of backpropagation\n",
    "The process can be described as a\n",
    "way of progressively correcting mistakes as soon as they are detected. Let's see how\n",
    "this works.\n",
    "\n",
    "Remember that each neural network layer has an associated set of weights that\n",
    "determine the output values for a given set of inputs. Additionally, remember that\n",
    "a neural network can have multiple hidden layers.\n",
    "\n",
    "At the beginning, all the weights have some random assignment. Then, the net is\n",
    "activated for each input in the training set: values are propagated forward from the\n",
    "input stage through the hidden stages to the output stage where a prediction is\n",
    "made.\n",
    "<img src=\"./i/Feedforward-Backpropagation-Neural-Network-architecture.png\" />\n",
    "Since we know the true observed value in the training set, it is possible to calculate\n",
    "the error made in prediction. The key intuition for backtracking is to propagate (spread and promote ) the\n",
    "error back , using an appropriate optimizer algorithm such as gradient\n",
    "descent to adjust the neural network weights with the goal of reducing the error.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The features represent the input, and the labels are used here to drive the learning\n",
    "process. The model is updated in such a way that the loss function is progressively\n",
    "minimized. In a neural network, what really matters is not the output of a single\n",
    "neuron but the collective weights adjusted in each layer. Therefore, the network\n",
    "progressively adjusts its internal weights in such a way that the prediction increases\n",
    "the number of correctly forecasted labels. Of course, using the right set of features\n",
    "and having quality labeled data is fundamental in order to minimize the bias during\n",
    "the learning process.\n",
    "<img src=\"./i/back_forwth.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-measure",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
