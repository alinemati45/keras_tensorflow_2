{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "living-nurse",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Neural-Network-Foundations-with-TensorFlow-2.0\" data-toc-modified-id=\"Neural-Network-Foundations-with-TensorFlow-2.0-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Neural Network Foundations with TensorFlow 2.0</a></span></li><li><span><a href=\"#Perceptron\" data-toc-modified-id=\"Perceptron-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Perceptron</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-first-example-of-TensorFlow-2.0-code\" data-toc-modified-id=\"A-first-example-of-TensorFlow-2.0-code-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>A first example of TensorFlow 2.0 code</a></span></li></ul></li><li><span><a href=\"#multi-layer-perceptron-(MLP)\" data-toc-modified-id=\"multi-layer-perceptron-(MLP)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>multi-layer perceptron (MLP)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problems-in-training-the-perceptron-and-their-solutions\" data-toc-modified-id=\"Problems-in-training-the-perceptron-and-their-solutions-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Problems in training the perceptron and their solutions</a></span></li><li><span><a href=\"#Can-we-do-without-an-activation-function?\" data-toc-modified-id=\"Can-we-do-without-an-activation-function?-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Can we do without an activation function?</a></span></li><li><span><a href=\"#What-is-Activation-Function?\" data-toc-modified-id=\"What-is-Activation-Function?-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>What is Activation Function?</a></span></li><li><span><a href=\"#Why-we-use-Activation-functions-with-Neural-Networks?\" data-toc-modified-id=\"Why-we-use-Activation-functions-with-Neural-Networks?-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Why we use Activation functions with Neural Networks?</a></span></li></ul></li><li><span><a href=\"#Linear-or-Identity-Activation-Function\" data-toc-modified-id=\"Linear-or-Identity-Activation-Function-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Linear or Identity Activation Function</a></span></li><li><span><a href=\"#Non-linear-Activation-Function\" data-toc-modified-id=\"Non-linear-Activation-Function-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Non-linear Activation Function</a></span></li><li><span><a href=\"#Activation-Function-sigmoid\" data-toc-modified-id=\"Activation-Function-sigmoid-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Activation Function sigmoid</a></span></li><li><span><a href=\"#Activation-function-‚Äì-tanh\" data-toc-modified-id=\"Activation-function-‚Äì-tanh-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Activation function ‚Äì tanh</a></span></li><li><span><a href=\"#ReLU-(Rectified-Linear-Unit)-Activation-Function\" data-toc-modified-id=\"ReLU-(Rectified-Linear-Unit)-Activation-Function-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>ReLU (Rectified Linear Unit) Activation Function</a></span></li><li><span><a href=\"#Leaky-ReLU\" data-toc-modified-id=\"Leaky-ReLU-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Leaky ReLU</a></span><ul class=\"toc-item\"><li><span><a href=\"#Why-derivative/differentiation-is-used-?\" data-toc-modified-id=\"Why-derivative/differentiation-is-used-?-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Why derivative/differentiation is used ?</a></span></li><li><span><a href=\"#An-example-of-an-activation-function-applied-after-a-linear-function\" data-toc-modified-id=\"An-example-of-an-activation-function-applied-after-a-linear-function-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>An example of an activation function applied after a linear function</a></span></li><li><span><a href=\"#what-are-neural-networks-after-all?\" data-toc-modified-id=\"what-are-neural-networks-after-all?-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>what are neural networks after all?</a></span></li></ul></li><li><span><a href=\"#A-real-example-‚Äì-recognizing-handwritten-digits\" data-toc-modified-id=\"A-real-example-‚Äì-recognizing-handwritten-digits-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>A real example ‚Äì recognizing handwritten digits</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-hot-encoding\" data-toc-modified-id=\"One-hot-encoding-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>One-hot encoding</a></span></li><li><span><a href=\"#EPOCH\" data-toc-modified-id=\"EPOCH-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>EPOCH</a></span></li><li><span><a href=\"#BATCH_SIZE\" data-toc-modified-id=\"BATCH_SIZE-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>BATCH_SIZE</a></span></li><li><span><a href=\"#VALIDATION\" data-toc-modified-id=\"VALIDATION-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;</span>VALIDATION</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-brick",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T05:39:30.421401Z",
     "start_time": "2021-08-09T05:39:30.405401Z"
    }
   },
   "source": [
    "# Neural Network Foundations with TensorFlow 2.0\n",
    "\n",
    "Why Tensorflow \n",
    "\n",
    "    ‚Ä¢ It works with all popular languages such as Python, C++, Java, R, and Go.\n",
    "    ‚Ä¢ Keras ‚Äì a high-level neural network API that has been integrated with TensorFlow (in 2.0, Keras became the standard API for interacting with TensorFlow). This API specifies how software components should interact.\n",
    "    ‚Ä¢ TensorFlow allows model deployment and ease of use in production.\n",
    "    ‚Ä¢ Support for eager computation (see Chapter 2, TensorFlow 1.x and 2.x) has been introduced in TensorFlow 2.0, in addition to graph computation based on static graphs.\n",
    "    ‚Ä¢ Most importantly, TensorFlow has very good community support.\n",
    "    \n",
    "What is Keras?\n",
    "\n",
    "Keras is a beautiful API for composing building blocks to create and train deep\n",
    "learning models. Keras is now part of TensorFlow. Another\n",
    "question is \"Should I use Keras or tf.keras?\" tf.keras is the implementation of\n",
    "Keras inside TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "facial-mills",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T17:27:20.873006Z",
     "start_time": "2021-08-09T17:27:20.839032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0.]\n",
      " [1. 0.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "W = tf.Variable(tf.ones(shape=(2,2)), name=\"W\")\n",
    "b = tf.Variable(tf.zeros(shape=(2)), name=\"b\")\n",
    "\n",
    "def model(x):\n",
    "    return W * x + b\n",
    "\n",
    "out_a = model([1,0])\n",
    "print(out_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-rehabilitation",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "The \"perceptron\" is a simple algorithm that, given an input vector x of m values (x1,x2,..., xm), often called input features or simply features, outputs either\n",
    "a 1 (\"yes\") or a 0 (\"no\"). \n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\n",
    "      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\n",
    "      \\end{array} \\right.\n",
    "\\tag{1}\\end{eqnarray}\n",
    "\n",
    "w is a vector of weights, wx is the dot product sigma ùë§ùëó * ùë•ùëó and b is bias. If you\n",
    "remember elementary geometry, wx + b defines a boundary hyperplane that changes\n",
    "position according to the values assigned to w and b. \n",
    "\n",
    "Note that a hyperplane is a subspace whose dimension is one less than that of its\n",
    "ambient space.\n",
    "\n",
    "## A first example of TensorFlow 2.0 code\n",
    "\n",
    "Each neuron can be initialized with specific weights via the kernel_initializer\n",
    "parameter. There are a few choices, the most common of which are listed as follows:\n",
    "1. random_uniform: Weights are initialized to uniformly random small values\n",
    "in the range -0.05 to 0.05.\n",
    "2. random_normal: Weights are initialized according to a Gaussian distribution,\n",
    "with zero mean and a small standard deviation of 0.05. For those of you who\n",
    "are not familiar with Gaussian distribution, think about a symmetric \"bell\n",
    "curve\" shape.\n",
    "3. zero: All weights are initialized to zero.\n",
    "\n",
    "\"perceptron\" was the name given to a model having one single linear\n",
    "layer, and as a consequence, if it has multiple layers, you would call it a multi-layer\n",
    "perceptron (MLP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "literary-wound",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T18:07:54.190898Z",
     "start_time": "2021-08-09T18:07:54.159080Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "NB_CLASSES = 10\n",
    "RESHAPED = 784\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "input_shape=(RESHAPED,), kernel_initializer='zeros', name='dense_layer', activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-respondent",
   "metadata": {},
   "source": [
    "#  multi-layer perceptron (MLP)\n",
    "Note that the input and the output layers are visible from outside, while all the other layers in the middle are hidden ‚Äì hence the name hidden layers.\n",
    "<img src=\"./i/A-hypothetical-example-of-Multilayer-Perceptron-Network.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-history",
   "metadata": {},
   "source": [
    "## Problems in training the perceptron and their solutions\n",
    "\n",
    "While the computer processes those images, we would like our neuron to adjust its weights\n",
    "and its bias so that we have fewer and fewer images wrongly recognized.\n",
    "\n",
    "we need a continuous function that allows us to compute the derivative.\n",
    "\n",
    "so, The activation functions help the network use the important information and suppress the irrelevant data points. not only 0 and 1 \n",
    "\n",
    "## Can we do without an activation function?\n",
    "We understand that using an activation function introduces an additional step at each layer during the forward propagation. Now the question is ‚Äì if the activation function increases the complexity so much, can we do without an activation function?\n",
    "\n",
    "Imagine a neural network without the activation functions. In that case, every neuron will only be performing a linear transformation on the inputs using the weights and biases. Although linear transformations make the neural network simpler, but this network would be less powerful and will not be able to learn the complex patterns from the data.\n",
    "\n",
    "## What is Activation Function?\n",
    "\n",
    "It‚Äôs just a thing function that you use to get the output of node. It is also known as Transfer Function.\n",
    "\n",
    "## Why we use Activation functions with Neural Networks?\n",
    "It is used to determine the output of neural network like yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. (depending upon the function).\n",
    "\n",
    "The Activation Functions can be basically divided into 2 types\n",
    "\n",
    "        Linear Activation Function\n",
    "        Non-linear Activation Functions\n",
    "\n",
    "# Linear or Identity Activation Function\n",
    "As you can see the function is a line or linear. Therefore, the output of the functions will not be confined between any range.\n",
    "\n",
    "\n",
    "<img src=\"./i/1_tldIgyDQWqm-sMwP7m3Bww.png\" />\n",
    "\n",
    "Equation : f(x) = x\n",
    "\n",
    "Range : (-infinity to infinity)\n",
    "\n",
    "It doesn‚Äôt help with the complexity or various parameters of usual data that is fed to the neural networks.\n",
    "\n",
    "# Non-linear Activation Function\n",
    "The Nonlinear Activation Functions are the most used activation functions. Nonlinearity helps to makes the graph look something like this\n",
    "\n",
    "\n",
    "# Activation Function sigmoid\n",
    "\n",
    "\\begin{eqnarray} \n",
    "  \\sigma(z) \\equiv \\frac{1}{1+e^{-z}}.\n",
    "\\tag{3}\\end{eqnarray}\n",
    "\n",
    "the range (0, 1) when the input varies in the range (‚àí‚àû, ‚àû). Mathematically the function is continuous.\n",
    "\n",
    "The Sigmoid Function curve looks like a S-shape.\n",
    "\n",
    "<img src=\"./i/1_Xu7B5y9gp0iL5ooBj7LtWw.png\" />\n",
    "\n",
    "The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
    "\n",
    "The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.\n",
    "\n",
    "The function is monotonic but function‚Äôs derivative is not.\n",
    "\n",
    "The logistic sigmoid function can cause a neural network to get stuck at the training time.\n",
    "\n",
    "The softmax function is a more generalized logistic activation function which is used for multiclass classification.\n",
    "\n",
    "\n",
    "A neuron can use the sigmoid for computing the nonlinear function ùúé(ùëß = ùë§x + ùëè).\n",
    "\n",
    "Note that if z = wx + b is very large and positive, then ùëí^‚àíùëß ‚Üí 0 so ùúé(ùëß) ‚Üí 1, \n",
    "\n",
    "while if z = wx + b is very large and negative ùëí^‚àíùëß ‚Üí ‚àû so ùúé(ùëß) ‚Üí 0. \n",
    "\n",
    "In other words, a neuron\n",
    "with sigmoid activation has a behavior similar to the perceptron, but the changes are\n",
    "gradual and output values such as 0.5539 or 0.123191 are perfectly legitimate. In this\n",
    "sense, a sigmoid neuron can answer \"maybe.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "assisted-massage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T18:31:45.229206Z",
     "start_time": "2021-08-09T18:31:45.148081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# example plot for the sigmoid activation function\n",
    "from math import exp\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# sigmoid activation function\n",
    "def sigmoid(x):\n",
    "\treturn 1.0 / (1.0 + exp(-x))\n",
    " \n",
    "# define input data\n",
    "inputs = [x for x in range(-10, 10)]\n",
    "# calculate outputs\n",
    "outputs = [sigmoid(x) for x in inputs]\n",
    "# plot inputs vs outputs\n",
    "pyplot.plot(inputs, outputs)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-lightning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T18:35:05.956663Z",
     "start_time": "2021-08-09T18:35:05.947637Z"
    }
   },
   "source": [
    "# Activation function ‚Äì tanh\n",
    "tanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).\n",
    "\n",
    "<img src=\"./i/1_f9erByySVjTjohfFdNkJYQ.jpeg\" />\n",
    "\n",
    "Hyperbolic Tangent Function: tanh(x) = (ex ‚Äì e-x) / (ex + e-x)\n",
    "\n",
    "\n",
    "The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.\n",
    "\n",
    "The function is differentiable.\n",
    "\n",
    "The function is monotonic while its derivative is not monotonic.\n",
    "\n",
    "The tanh function is mainly used classification between two classes.\n",
    "\n",
    "Both tanh and logistic sigmoid activation functions are used in feed-forward nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "preceding-narrative",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T18:36:42.622084Z",
     "start_time": "2021-08-09T18:36:42.540088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example plot for the tanh activation function\n",
    "from math import exp\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# tanh activation function\n",
    "def tanh(x):\n",
    "\treturn (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "# define input data\n",
    "inputs = [x for x in range(-10, 10)]\n",
    "# calculate outputs\n",
    "outputs = [tanh(x) for x in inputs]\n",
    "# plot inputs vs outputs\n",
    "pyplot.plot(inputs, outputs)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-terminal",
   "metadata": {},
   "source": [
    "#  ReLU (Rectified Linear Unit) Activation Function\n",
    "The ReLU is the most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning.\n",
    "\n",
    "A ReLU is simply defined as f(x) =max(0, x) \n",
    "\n",
    "<img src=\"./i/1_XxxiA0jJvPrHEJHD4z893g.png\" />\n",
    "\n",
    "As you can see, the ReLU is half rectified (from bottom). f(z) is zero when z is less than zero and f(z) is equal to z when z is above or equal to zero.\n",
    "Range: [ 0 to infinity)\n",
    "\n",
    "The function and its derivative both are monotonic.\n",
    "\n",
    "But the issue is that all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly. \n",
    "\n",
    "That means any negative input given to the ReLU activation function turns the value into zero immediately in the graph, which in turns affects the resulting graph by not mapping the negative values appropriately.\n",
    "\n",
    "**very popular because it helps address some optimization problems observed with sigmoids.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-treat",
   "metadata": {},
   "source": [
    "# Leaky ReLU\n",
    "It is an attempt to solve the dying ReLU problem\n",
    "\n",
    "<img src=\"./i/1_A_Bzn0CjUgOXtPCJKnKLqA.jpeg\" />\n",
    "\n",
    "\n",
    "The leak helps to increase the range of the ReLU function. Usually, the value of a is 0.01 or so.\n",
    "When a is not 0.01 then it is called Randomized ReLU.\n",
    "\n",
    "Therefore the range of the Leaky ReLU is (-infinity to infinity).\n",
    "\n",
    "Both Leaky and Randomized ReLU functions are monotonic in nature. Also, their derivatives also monotonic in nature.\n",
    "\n",
    "\n",
    "<img src=\"./i/0_lo8wlkwReDcXkts0.png\" />\n",
    "     \n",
    "     \n",
    "## Why derivative/differentiation is used ?\n",
    "When updating the curve, to know in which direction and how much to change or update the curve depending upon the slope.That is why we use differentiation in almost every part of Machine Learning and Deep Learning.\n",
    "\n",
    "\n",
    "## An example of an activation function applied after a linear function\n",
    "<img src=\"./i/B14761_01_13.png\" />\n",
    "\n",
    "\n",
    "## what are neural networks after all?\n",
    "\n",
    "machine learning models are a way to compute a function that maps\n",
    "some inputs to their corresponding outputs. The function is nothing more than a\n",
    "number of addition and multiplication operations. However, when combined with\n",
    "a non-linear activation and stacked in multiple layers, these functions can learn\n",
    "almost anything.\n",
    "\n",
    "\n",
    "# A real example ‚Äì recognizing handwritten digits\n",
    "\n",
    "## One-hot encoding \n",
    "to transform categorical (nonnumerical) features into numerical variables. For instance, the categorical feature\n",
    "\"digit\" with value d in [0 ‚Äì 9] can be encoded into a binary vector with 10 positions,\n",
    "which always has 0 value except the d - th position where a 1 is present.\n",
    "\n",
    "## EPOCH \n",
    "defines how long the training should last, \n",
    "## BATCH_SIZE \n",
    "is the number of samples you feed in to your network at a time, \n",
    "## VALIDATION \n",
    "is the amount of data reserved for checking or proving the validity of the training process.\n",
    "\n",
    "The reason why we picked EPOCHS = 200, BATCH_SIZE = 128, VALIDATION_\n",
    "SPLIT=0.2, and N_HIDDEN = 128 will be clearer later in this chapter when we will\n",
    "explore different values and discuss hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "editorial-beginning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T18:53:56.958010Z",
     "start_time": "2021-08-09T18:53:56.949011Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# network and training parameters\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of output \n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "N_HIDDEN = 128\n",
    "# Loading MNIST dataset.\n",
    "# verify\n",
    "# You can verify that the split between train and test is 60,000, and 0,000 respectively.\n",
    "# Labels have one-hot representation.is automatically applied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "strange-birth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T19:00:05.546889Z",
     "start_time": "2021-08-09T19:00:05.355860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,), 10000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(X_train , Y_train) , (X_test , Y_test) = mnist.load_data()\n",
    "X_train.shape , Y_train.shape , X_test.shape , Y_test.shape ,  X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "indonesian-warning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T19:00:06.278566Z",
     "start_time": "2021-08-09T19:00:06.220407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train is 60000 rows of 28x28 values;\n",
    "# we --> reshape it to 60000 x 784.\n",
    "print(28*28)\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000 , RESHAPED).astype('float32')\n",
    "X_test = X_test.reshape( X_test.shape[0], RESHAPED).astype('float32')\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "rational-sullivan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T19:00:33.936964Z",
     "start_time": "2021-08-09T19:00:33.915965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Normalize inputs to be within in [0, 1].\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "coastal-credit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T19:00:35.617806Z",
     "start_time": "2021-08-09T19:00:35.605771Z"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot representation of the labels.\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "further-cherry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T19:04:42.618239Z",
     "start_time": "2021-08-09T19:04:42.574242Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the model.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(NB_CLASSES , \n",
    "                             input_shape = (RESHAPED ,),\n",
    "                             name='dense_layer' , activation='softmax'\n",
    "                             \n",
    "                            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-conclusion",
   "metadata": {},
   "source": [
    "Once we define the model, we have to compile it so that it can be executed by\n",
    "TensorFlow 2.0. There are a few choices to be made during compilation. \n",
    "\n",
    "Firstly, we need to select an optimizer, which is the specific algorithm used to update\n",
    "weights while we train our model. \n",
    "\n",
    "Second, we need to select an objective function, which is used by the optimizer to navigate the space of weights (frequently, objective functions are called either loss functions or cost functions and the process of\n",
    "optimization is defined as a process of loss minimization). Typically, with neural networks, we seek to minimize the error. As such, the objective function is often referred to as a cost function or a loss function and the value calculated by the loss function is referred to as simply ‚Äúloss.‚Äù\n",
    "\n",
    " \n",
    "\n",
    "Third, we need to evaluate the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "virgin-witch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T19:09:05.883367Z",
     "start_time": "2021-08-09T19:09:05.843398Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compiling the model.\n",
    "model.compile(optimizer='SGD',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "guided-statistics",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T19:11:01.725261Z",
     "start_time": "2021-08-09T19:09:13.380513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s 31us/sample - loss: 2.3020 - accuracy: 0.1120 - val_loss: 2.3019 - val_accuracy: 0.1060\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.3014 - accuracy: 0.1140 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.3010 - accuracy: 0.1140 - val_loss: 2.3016 - val_accuracy: 0.1060\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.3008 - accuracy: 0.1140 - val_loss: 2.3016 - val_accuracy: 0.1060\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.3007 - accuracy: 0.1140 - val_loss: 2.3015 - val_accuracy: 0.1060\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.3006 - accuracy: 0.1140 - val_loss: 2.3015 - val_accuracy: 0.1060\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.3005 - accuracy: 0.1140 - val_loss: 2.3015 - val_accuracy: 0.1060\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.3005 - accuracy: 0.1140 - val_loss: 2.3014 - val_accuracy: 0.1060\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.3004 - accuracy: 0.1140 - val_loss: 2.3014 - val_accuracy: 0.1060\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.3003 - accuracy: 0.1140 - val_loss: 2.3013 - val_accuracy: 0.1060\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.3003 - accuracy: 0.1140 - val_loss: 2.3013 - val_accuracy: 0.1060\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.3002 - accuracy: 0.1140 - val_loss: 2.3012 - val_accuracy: 0.1060\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.3001 - accuracy: 0.1140 - val_loss: 2.3012 - val_accuracy: 0.1060\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.3001 - accuracy: 0.1140 - val_loss: 2.3011 - val_accuracy: 0.1060\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.3000 - accuracy: 0.1140 - val_loss: 2.3010 - val_accuracy: 0.1060\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.3000 - accuracy: 0.1140 - val_loss: 2.3010 - val_accuracy: 0.1060\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2999 - accuracy: 0.1140 - val_loss: 2.3009 - val_accuracy: 0.1060\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2998 - accuracy: 0.1140 - val_loss: 2.3008 - val_accuracy: 0.1060\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2998 - accuracy: 0.1140 - val_loss: 2.3008 - val_accuracy: 0.1060\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2997 - accuracy: 0.1140 - val_loss: 2.3007 - val_accuracy: 0.1060\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2996 - accuracy: 0.1140 - val_loss: 2.3006 - val_accuracy: 0.1060\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2996 - accuracy: 0.1140 - val_loss: 2.3006 - val_accuracy: 0.1060\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2995 - accuracy: 0.1140 - val_loss: 2.3005 - val_accuracy: 0.1060\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2994 - accuracy: 0.1140 - val_loss: 2.3004 - val_accuracy: 0.1060\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2994 - accuracy: 0.1140 - val_loss: 2.3004 - val_accuracy: 0.1060\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2993 - accuracy: 0.1140 - val_loss: 2.3003 - val_accuracy: 0.1060\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2993 - accuracy: 0.1140 - val_loss: 2.3002 - val_accuracy: 0.1060\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2992 - accuracy: 0.1140 - val_loss: 2.3002 - val_accuracy: 0.1060\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2991 - accuracy: 0.1140 - val_loss: 2.3001 - val_accuracy: 0.1060\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2991 - accuracy: 0.1140 - val_loss: 2.3000 - val_accuracy: 0.1060\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2990 - accuracy: 0.1140 - val_loss: 2.3000 - val_accuracy: 0.1060\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2989 - accuracy: 0.1140 - val_loss: 2.2999 - val_accuracy: 0.1060\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2989 - accuracy: 0.1140 - val_loss: 2.2998 - val_accuracy: 0.1060\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2988 - accuracy: 0.1140 - val_loss: 2.2998 - val_accuracy: 0.1060\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2987 - accuracy: 0.1140 - val_loss: 2.2997 - val_accuracy: 0.1060\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2987 - accuracy: 0.1140 - val_loss: 2.2996 - val_accuracy: 0.1060\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2986 - accuracy: 0.1140 - val_loss: 2.2996 - val_accuracy: 0.1060\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2986 - accuracy: 0.1140 - val_loss: 2.2995 - val_accuracy: 0.1060\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2985 - accuracy: 0.1140 - val_loss: 2.2995 - val_accuracy: 0.1060\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2984 - accuracy: 0.1140 - val_loss: 2.2994 - val_accuracy: 0.1060\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2984 - accuracy: 0.1140 - val_loss: 2.2993 - val_accuracy: 0.1060\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2983 - accuracy: 0.1140 - val_loss: 2.2993 - val_accuracy: 0.1060\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2982 - accuracy: 0.1140 - val_loss: 2.2992 - val_accuracy: 0.1060\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2982 - accuracy: 0.1140 - val_loss: 2.2991 - val_accuracy: 0.1060\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2981 - accuracy: 0.1140 - val_loss: 2.2991 - val_accuracy: 0.1060\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2981 - accuracy: 0.1140 - val_loss: 2.2990 - val_accuracy: 0.1060\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2980 - accuracy: 0.1140 - val_loss: 2.2989 - val_accuracy: 0.1060\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2979 - accuracy: 0.1140 - val_loss: 2.2989 - val_accuracy: 0.1060\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2979 - accuracy: 0.1140 - val_loss: 2.2988 - val_accuracy: 0.1060\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2978 - accuracy: 0.1140 - val_loss: 2.2988 - val_accuracy: 0.1060\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2977 - accuracy: 0.1140 - val_loss: 2.2987 - val_accuracy: 0.1060\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2977 - accuracy: 0.1140 - val_loss: 2.2986 - val_accuracy: 0.1060\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2976 - accuracy: 0.1140 - val_loss: 2.2986 - val_accuracy: 0.1060\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2975 - accuracy: 0.1140 - val_loss: 2.2985 - val_accuracy: 0.1060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2975 - accuracy: 0.1140 - val_loss: 2.2984 - val_accuracy: 0.1060\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2974 - accuracy: 0.1140 - val_loss: 2.2984 - val_accuracy: 0.1060\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2974 - accuracy: 0.1140 - val_loss: 2.2983 - val_accuracy: 0.1060\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2973 - accuracy: 0.1140 - val_loss: 2.2982 - val_accuracy: 0.1060\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2972 - accuracy: 0.1140 - val_loss: 2.2982 - val_accuracy: 0.1060\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2972 - accuracy: 0.1140 - val_loss: 2.2981 - val_accuracy: 0.1060\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2971 - accuracy: 0.1140 - val_loss: 2.2980 - val_accuracy: 0.1060\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2970 - accuracy: 0.1140 - val_loss: 2.2980 - val_accuracy: 0.1060\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2970 - accuracy: 0.1140 - val_loss: 2.2979 - val_accuracy: 0.1060\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2969 - accuracy: 0.1140 - val_loss: 2.2978 - val_accuracy: 0.1060\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2968 - accuracy: 0.1140 - val_loss: 2.2978 - val_accuracy: 0.1060\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2968 - accuracy: 0.1140 - val_loss: 2.2977 - val_accuracy: 0.1060\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2967 - accuracy: 0.1140 - val_loss: 2.2976 - val_accuracy: 0.1060\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2967 - accuracy: 0.1140 - val_loss: 2.2976 - val_accuracy: 0.1060\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2966 - accuracy: 0.1140 - val_loss: 2.2975 - val_accuracy: 0.1060\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2965 - accuracy: 0.1140 - val_loss: 2.2975 - val_accuracy: 0.1060\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2965 - accuracy: 0.1140 - val_loss: 2.2974 - val_accuracy: 0.1060\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2964 - accuracy: 0.1140 - val_loss: 2.2973 - val_accuracy: 0.1060\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2963 - accuracy: 0.1140 - val_loss: 2.2973 - val_accuracy: 0.1060\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2963 - accuracy: 0.1140 - val_loss: 2.2972 - val_accuracy: 0.1060\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2962 - accuracy: 0.1140 - val_loss: 2.2971 - val_accuracy: 0.1060\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2961 - accuracy: 0.1140 - val_loss: 2.2971 - val_accuracy: 0.1060\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2961 - accuracy: 0.1140 - val_loss: 2.2970 - val_accuracy: 0.1060\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2960 - accuracy: 0.1140 - val_loss: 2.2969 - val_accuracy: 0.1060\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2960 - accuracy: 0.1140 - val_loss: 2.2969 - val_accuracy: 0.1060\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2959 - accuracy: 0.1140 - val_loss: 2.2968 - val_accuracy: 0.1060\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2958 - accuracy: 0.1140 - val_loss: 2.2968 - val_accuracy: 0.1060\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2958 - accuracy: 0.1140 - val_loss: 2.2967 - val_accuracy: 0.1060\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2957 - accuracy: 0.1140 - val_loss: 2.2966 - val_accuracy: 0.1060\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2956 - accuracy: 0.1140 - val_loss: 2.2966 - val_accuracy: 0.1060\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2956 - accuracy: 0.1140 - val_loss: 2.2965 - val_accuracy: 0.1060\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2955 - accuracy: 0.1140 - val_loss: 2.2964 - val_accuracy: 0.1060\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2955 - accuracy: 0.1140 - val_loss: 2.2964 - val_accuracy: 0.1060\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2954 - accuracy: 0.1140 - val_loss: 2.2963 - val_accuracy: 0.1060\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2953 - accuracy: 0.1140 - val_loss: 2.2962 - val_accuracy: 0.1060\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2953 - accuracy: 0.1140 - val_loss: 2.2962 - val_accuracy: 0.1060\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2952 - accuracy: 0.1140 - val_loss: 2.2961 - val_accuracy: 0.1060\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2951 - accuracy: 0.1140 - val_loss: 2.2960 - val_accuracy: 0.1060\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2951 - accuracy: 0.1140 - val_loss: 2.2960 - val_accuracy: 0.1060\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2950 - accuracy: 0.1140 - val_loss: 2.2959 - val_accuracy: 0.1060\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2949 - accuracy: 0.1140 - val_loss: 2.2958 - val_accuracy: 0.1060\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2949 - accuracy: 0.1140 - val_loss: 2.2958 - val_accuracy: 0.1060\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2948 - accuracy: 0.1140 - val_loss: 2.2957 - val_accuracy: 0.1060\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2948 - accuracy: 0.1140 - val_loss: 2.2957 - val_accuracy: 0.1060\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2947 - accuracy: 0.1140 - val_loss: 2.2956 - val_accuracy: 0.1060\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2946 - accuracy: 0.1140 - val_loss: 2.2955 - val_accuracy: 0.1060\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2946 - accuracy: 0.1140 - val_loss: 2.2955 - val_accuracy: 0.1060\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2945 - accuracy: 0.1140 - val_loss: 2.2954 - val_accuracy: 0.1060\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2944 - accuracy: 0.1140 - val_loss: 2.2953 - val_accuracy: 0.1060\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2944 - accuracy: 0.1140 - val_loss: 2.2953 - val_accuracy: 0.1060\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2943 - accuracy: 0.1140 - val_loss: 2.2952 - val_accuracy: 0.1060\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2943 - accuracy: 0.1140 - val_loss: 2.2951 - val_accuracy: 0.1060\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2942 - accuracy: 0.1140 - val_loss: 2.2951 - val_accuracy: 0.1060\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2941 - accuracy: 0.1140 - val_loss: 2.2950 - val_accuracy: 0.1060\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2941 - accuracy: 0.1140 - val_loss: 2.2949 - val_accuracy: 0.1060\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2940 - accuracy: 0.1140 - val_loss: 2.2949 - val_accuracy: 0.1060\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2939 - accuracy: 0.1140 - val_loss: 2.2948 - val_accuracy: 0.1060\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2939 - accuracy: 0.1140 - val_loss: 2.2947 - val_accuracy: 0.1060\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2938 - accuracy: 0.1140 - val_loss: 2.2947 - val_accuracy: 0.1060\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2937 - accuracy: 0.1140 - val_loss: 2.2946 - val_accuracy: 0.1060\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2937 - accuracy: 0.1140 - val_loss: 2.2945 - val_accuracy: 0.1060\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2936 - accuracy: 0.1140 - val_loss: 2.2945 - val_accuracy: 0.1060\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2936 - accuracy: 0.1140 - val_loss: 2.2944 - val_accuracy: 0.1060\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2935 - accuracy: 0.1140 - val_loss: 2.2944 - val_accuracy: 0.1060\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2934 - accuracy: 0.1140 - val_loss: 2.2943 - val_accuracy: 0.1060\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2934 - accuracy: 0.1140 - val_loss: 2.2942 - val_accuracy: 0.1060\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2933 - accuracy: 0.1140 - val_loss: 2.2942 - val_accuracy: 0.1060\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2932 - accuracy: 0.1140 - val_loss: 2.2941 - val_accuracy: 0.1060\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2932 - accuracy: 0.1140 - val_loss: 2.2940 - val_accuracy: 0.1060\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2931 - accuracy: 0.1140 - val_loss: 2.2940 - val_accuracy: 0.1060\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2931 - accuracy: 0.1140 - val_loss: 2.2939 - val_accuracy: 0.1060\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2930 - accuracy: 0.1140 - val_loss: 2.2939 - val_accuracy: 0.1060\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2929 - accuracy: 0.1140 - val_loss: 2.2938 - val_accuracy: 0.1060\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2929 - accuracy: 0.1140 - val_loss: 2.2937 - val_accuracy: 0.1060\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2928 - accuracy: 0.1140 - val_loss: 2.2937 - val_accuracy: 0.1060\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2927 - accuracy: 0.1140 - val_loss: 2.2936 - val_accuracy: 0.1060\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2927 - accuracy: 0.1140 - val_loss: 2.2935 - val_accuracy: 0.1060\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2926 - accuracy: 0.1140 - val_loss: 2.2935 - val_accuracy: 0.1060\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2925 - accuracy: 0.1140 - val_loss: 2.2934 - val_accuracy: 0.1060\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2925 - accuracy: 0.1140 - val_loss: 2.2933 - val_accuracy: 0.1060\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2924 - accuracy: 0.1140 - val_loss: 2.2933 - val_accuracy: 0.1060\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2924 - accuracy: 0.1140 - val_loss: 2.2932 - val_accuracy: 0.1060\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2923 - accuracy: 0.1140 - val_loss: 2.2932 - val_accuracy: 0.1060\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2922 - accuracy: 0.1140 - val_loss: 2.2931 - val_accuracy: 0.1060\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2922 - accuracy: 0.1140 - val_loss: 2.2930 - val_accuracy: 0.1060\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2921 - accuracy: 0.1140 - val_loss: 2.2930 - val_accuracy: 0.1060\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2920 - accuracy: 0.1140 - val_loss: 2.2929 - val_accuracy: 0.1060\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2920 - accuracy: 0.1140 - val_loss: 2.2928 - val_accuracy: 0.1060\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2919 - accuracy: 0.1140 - val_loss: 2.2928 - val_accuracy: 0.1060\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2918 - accuracy: 0.1140 - val_loss: 2.2927 - val_accuracy: 0.1060\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2918 - accuracy: 0.1140 - val_loss: 2.2926 - val_accuracy: 0.1060\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2917 - accuracy: 0.1140 - val_loss: 2.2926 - val_accuracy: 0.1060\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2917 - accuracy: 0.1140 - val_loss: 2.2925 - val_accuracy: 0.1060\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2916 - accuracy: 0.1140 - val_loss: 2.2924 - val_accuracy: 0.1060\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2915 - accuracy: 0.1140 - val_loss: 2.2924 - val_accuracy: 0.1060\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2915 - accuracy: 0.1140 - val_loss: 2.2923 - val_accuracy: 0.1060\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2914 - accuracy: 0.1140 - val_loss: 2.2923 - val_accuracy: 0.1060\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2913 - accuracy: 0.1140 - val_loss: 2.2922 - val_accuracy: 0.1060\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2913 - accuracy: 0.1140 - val_loss: 2.2921 - val_accuracy: 0.1060\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2912 - accuracy: 0.1140 - val_loss: 2.2921 - val_accuracy: 0.1060\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2912 - accuracy: 0.1140 - val_loss: 2.2920 - val_accuracy: 0.1060\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2911 - accuracy: 0.1140 - val_loss: 2.2919 - val_accuracy: 0.1060\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2910 - accuracy: 0.1140 - val_loss: 2.2919 - val_accuracy: 0.1060\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2910 - accuracy: 0.1140 - val_loss: 2.2918 - val_accuracy: 0.1060\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2909 - accuracy: 0.1140 - val_loss: 2.2917 - val_accuracy: 0.1060\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2908 - accuracy: 0.1140 - val_loss: 2.2917 - val_accuracy: 0.1060\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2908 - accuracy: 0.1140 - val_loss: 2.2916 - val_accuracy: 0.1060\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2907 - accuracy: 0.1140 - val_loss: 2.2915 - val_accuracy: 0.1060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2907 - accuracy: 0.1140 - val_loss: 2.2915 - val_accuracy: 0.1060\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2906 - accuracy: 0.1140 - val_loss: 2.2914 - val_accuracy: 0.1060\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2905 - accuracy: 0.1140 - val_loss: 2.2913 - val_accuracy: 0.1060\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2905 - accuracy: 0.1140 - val_loss: 2.2913 - val_accuracy: 0.1060\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2904 - accuracy: 0.1140 - val_loss: 2.2912 - val_accuracy: 0.1060\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2903 - accuracy: 0.1140 - val_loss: 2.2912 - val_accuracy: 0.1060\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2903 - accuracy: 0.1140 - val_loss: 2.2911 - val_accuracy: 0.1060\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2902 - accuracy: 0.1140 - val_loss: 2.2910 - val_accuracy: 0.1060\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2901 - accuracy: 0.1140 - val_loss: 2.2910 - val_accuracy: 0.1060\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2901 - accuracy: 0.1140 - val_loss: 2.2909 - val_accuracy: 0.1060\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2900 - accuracy: 0.1140 - val_loss: 2.2908 - val_accuracy: 0.1060\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2900 - accuracy: 0.1140 - val_loss: 2.2908 - val_accuracy: 0.1060\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2899 - accuracy: 0.1140 - val_loss: 2.2907 - val_accuracy: 0.1060\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2898 - accuracy: 0.1140 - val_loss: 2.2907 - val_accuracy: 0.1060\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2898 - accuracy: 0.1140 - val_loss: 2.2906 - val_accuracy: 0.1060\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2897 - accuracy: 0.1140 - val_loss: 2.2905 - val_accuracy: 0.1060\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2896 - accuracy: 0.1140 - val_loss: 2.2905 - val_accuracy: 0.1060\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2896 - accuracy: 0.1140 - val_loss: 2.2904 - val_accuracy: 0.1060\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2895 - accuracy: 0.1140 - val_loss: 2.2903 - val_accuracy: 0.1060\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2895 - accuracy: 0.1140 - val_loss: 2.2903 - val_accuracy: 0.1060\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2894 - accuracy: 0.1140 - val_loss: 2.2902 - val_accuracy: 0.1060\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2893 - accuracy: 0.1140 - val_loss: 2.2901 - val_accuracy: 0.1060\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2893 - accuracy: 0.1140 - val_loss: 2.2901 - val_accuracy: 0.1060\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2892 - accuracy: 0.1140 - val_loss: 2.2900 - val_accuracy: 0.1060\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2891 - accuracy: 0.1140 - val_loss: 2.2899 - val_accuracy: 0.1060\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2891 - accuracy: 0.1140 - val_loss: 2.2899 - val_accuracy: 0.1060\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2890 - accuracy: 0.1140 - val_loss: 2.2898 - val_accuracy: 0.1060\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2889 - accuracy: 0.1140 - val_loss: 2.2897 - val_accuracy: 0.1060\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 2.2889 - accuracy: 0.1140 - val_loss: 2.2897 - val_accuracy: 0.1060\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2888 - accuracy: 0.1140 - val_loss: 2.2896 - val_accuracy: 0.1060\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2888 - accuracy: 0.1140 - val_loss: 2.2895 - val_accuracy: 0.1060\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2887 - accuracy: 0.1140 - val_loss: 2.2895 - val_accuracy: 0.1060\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2886 - accuracy: 0.1140 - val_loss: 2.2894 - val_accuracy: 0.1060\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2886 - accuracy: 0.1140 - val_loss: 2.2894 - val_accuracy: 0.1060\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2885 - accuracy: 0.1140 - val_loss: 2.2893 - val_accuracy: 0.1060\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2884 - accuracy: 0.1140 - val_loss: 2.2892 - val_accuracy: 0.1060\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2884 - accuracy: 0.1140 - val_loss: 2.2892 - val_accuracy: 0.1060\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 2.2883 - accuracy: 0.1140 - val_loss: 2.2891 - val_accuracy: 0.1060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x275eb02bf88>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model.\n",
    "model.fit(X_train, Y_train,\n",
    "            batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "            verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "#evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "minute-protein",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-09T19:12:41.476298Z",
     "start_time": "2021-08-09T19:12:41.461217Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.1135\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-shepherd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
