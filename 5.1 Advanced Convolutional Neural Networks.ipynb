{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "provincial-prediction",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Advanced-Convolutional-Neural-Networks\" data-toc-modified-id=\"Advanced-Convolutional-Neural-Networks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Advanced Convolutional Neural Networks</a></span></li><li><span><a href=\"#Classification-and-localization\" data-toc-modified-id=\"Classification-and-localization-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Classification and localization</a></span></li><li><span><a href=\"#Semantic-segmentation\" data-toc-modified-id=\"Semantic-segmentation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Semantic segmentation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Approch-1-and-2\" data-toc-modified-id=\"Approch-1-and-2-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Approch 1 and 2</a></span></li><li><span><a href=\"#Approch--CNN-encoder-decoder-network\" data-toc-modified-id=\"Approch--CNN-encoder-decoder-network-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Approch  CNN encoder-decoder network</a></span></li></ul></li><li><span><a href=\"#Object-detection\" data-toc-modified-id=\"Object-detection-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Object detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#classification-and-localization-networks\" data-toc-modified-id=\"classification-and-localization-networks-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>classification and localization networks</a></span></li><li><span><a href=\"#Selective-Search---Region-Proposals-CNN-(R-CNN)\" data-toc-modified-id=\"Selective-Search---Region-Proposals-CNN-(R-CNN)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Selective Search - Region Proposals-CNN (R-CNN)</a></span></li><li><span><a href=\"#SVM-based-classifier\" data-toc-modified-id=\"SVM-based-classifier-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>SVM-based classifier</a></span></li><li><span><a href=\"#Fast-R-CNN\" data-toc-modified-id=\"Fast-R-CNN-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Fast R-CNN</a></span></li><li><span><a href=\"#Faster-R-CNN\" data-toc-modified-id=\"Faster-R-CNN-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Faster R-CNN</a></span></li><li><span><a href=\"#You-Only-Look-Once-(YOLO)\" data-toc-modified-id=\"You-Only-Look-Once-(YOLO)-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>You Only Look Once (YOLO)</a></span></li></ul></li><li><span><a href=\"#Instance-segmentation\" data-toc-modified-id=\"Instance-segmentation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Instance segmentation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-buffalo",
   "metadata": {},
   "source": [
    "# Advanced Convolutional Neural Networks\n",
    "\n",
    "Different computer vision tasks. Source: Introduction to Artificial Intelligence and Computer\n",
    "Vision Revolution (https://www.slideshare.net/darian_f/introduction-to-the-artificial-intelligence-andcomputer-vision-revolution)\n",
    "\n",
    "![](./i/computer_vision_tasks.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-foundation",
   "metadata": {},
   "source": [
    "# Classification and localization\n",
    "\n",
    "In the classification and localization task not only do you have to report the class of\n",
    "object found in the image, but also the coordinates of the bounding box where the\n",
    "object appears in the image. This type of task assumes that there is only one instance\n",
    "of the object in an image.\n",
    "\n",
    "*This can be achieved by attaching a \"regression head\"  in addition to the\n",
    "\"classification head\" in a typical classification network *\n",
    "\n",
    "    - classification head:   \n",
    "    Recall that in a classification network, the final output of convolution and pooling operations, called the feature map, is fed into a fully connected network that produces a vector of class probabilities. This fully connected network is called the `classification head`, and it is tuned using a categorical loss function (Lc) such as `categorical cross entropy`.\n",
    "\n",
    "    - regression head:\n",
    "    a regression head is another fully connected network that takes the feature map and produces a vector (x, y, w, h) representing the top-left x and y coordinates, width and height of the bounding box. \n",
    "    It is tuned using a continuous loss function (Lr) such as mean squared error. The entire network is tuned using a linear combination of the two losses, that is:\n",
    "$$\n",
    "ùêø = ùõºùêø_C + (1 ‚àí ùõº)ùêø_ùëü\n",
    "$$\n",
    "\n",
    "Here $ùõº$ is a hyperparameter and can take a value between 0 and 1. Unless the value\n",
    "is determined by some domain knowledge about the problem, it can be set to 0.5.\n",
    "\n",
    "![](./i/localizationandclassification_pattern.png)\n",
    "\n",
    "*the only difference with respect to a typical CNN classification network is the additional regression head on the top right*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-labor",
   "metadata": {},
   "source": [
    "# Semantic segmentation\n",
    "the aim is to classify every single pixel on the image as belonging to a single class.\n",
    "\n",
    "## Approch 1 and 2\n",
    "\n",
    "An initial method of implementation could be to build a classifier network for each\n",
    "pixel, where the input is a small neighborhood around each pixel. In practice, this\n",
    "approach is not very performant, so an improvement over this implementation\n",
    "might be to run the image through convolutions that will increase the feature depth,\n",
    "while keeping the image width and height constant. Each pixel then has a feature\n",
    "map that can be sent through a fully connected network that predicts the class of the\n",
    "pixel. However, in practice, this is also quite expensive, and it is not normally used.\n",
    "\n",
    "## Approch  CNN encoder-decoder network\n",
    "the `encoder` decreases the width and height of the image but increases its depth (number of\n",
    "features), while the `decoder` uses transposed convolution operations to increase its\n",
    "size and decrease depth. Transpose convolution (or upsampling) is the process of\n",
    "going in the opposite direction of a normal convolution. The input to this network is\n",
    "the image and the output is the segmentation map.\n",
    "\n",
    "A popular implementation of this encoder-decoder architecture is the U-Net. (https://github.com/jakeret/tf_unet)\n",
    "\n",
    "U-Net: Convolutional Networks for Biomedical Image Segmentation:\n",
    "![](./i/u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-warning",
   "metadata": {},
   "source": [
    "# Object detection\n",
    "The object detection task is similar to the classification and localization tasks. The big\n",
    "difference is that now there are multiple objects in the image, and for each one we\n",
    "need to find the class and bounding box coordinates. In addition, neither the number\n",
    "of objects nor their size is known in advance. As you can imagine, this is a difficult\n",
    "problem and a fair amount of research has gone into it.\n",
    "\n",
    "\n",
    "##  classification and localization networks\n",
    "A first approach to the problem might be to create many random crops of the\n",
    "input image and for each crop, apply the classification and localization networks\n",
    "we described earlier. However, such an approach is very wasteful in terms of\n",
    "computing and unlikely to be very successful.\n",
    "\n",
    "## Selective Search - Region Proposals-CNN (R-CNN)\n",
    "A more practical approach would be use a tool such as Selective Search (Selective\n",
    "Search for Object Recognition, by Uijlings et al.\n",
    "(http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)\n",
    "\n",
    "uses traditional computer\n",
    "vision techniques to find areas in the image that might contain objects. These regions\n",
    "are called \"Region Proposals,\" and the network to detect them was called \"Region\n",
    "Proposal Network,\" or R-CNN. In the original R-CNN, the regions were resized\n",
    "and fed into a network to yield image vectors:\n",
    "\n",
    "![](./i/R_cnn_cnn.png)\n",
    "\n",
    "## SVM-based classifier\n",
    "These vectors were then classified with an `SVM-based classifier` (https://\n",
    "en.wikipedia.org/wiki/Support-vector_machine), and the bounding boxes\n",
    "proposed by the external tool were corrected using a linear regression network over\n",
    "the image vectors. A R-CNN network can be represented conceptually as shown in\n",
    "Figure 5:\n",
    "\n",
    "![](./i/R_cnn_Network.png)\n",
    "\n",
    "## Fast R-CNN\n",
    "The next iteration of the R-CNN network was called the `Fast R-CNN`. The Fast\n",
    "R-CNN still gets its region proposals from an external tool, but instead of feeding\n",
    "each region proposal through the CNN, the entire image is fed through the CNN\n",
    "and the region proposals are projected onto the resulting feature map. Each region of\n",
    "interest is fed through an **Region of Interest (ROI)** pooling layer and then to a fully\n",
    "connected network, which produces a feature vector for the ROI. \n",
    "\n",
    "ROI pooling is a widely used operation in object detection tasks using convolutional\n",
    "neural networks. The ROI pooling layer uses max pooling to convert the features\n",
    "inside any valid region of interest into a small feature map with a fixed spatial extent\n",
    "of H √ó W (where H and W are two hyperparameters). The feature vector is then\n",
    "fed into two fully connected networks, one to predict the class of the ROI and the\n",
    "other to correct the bounding box coordinates for the proposal. This is illustrated\n",
    "in Figure 6:\n",
    "\n",
    "![](./i/fast_Rnn.png)\n",
    "\n",
    "## Faster R-CNN\n",
    "The Fast R-CNN is about 25x faster than the R-CNN. The next improvement, called\n",
    "the Faster R-CNN (an implementation can be found at https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN), removes the\n",
    "external region proposal mechanism and replaces it with a trainable component,\n",
    "called the Region Proposal Network (RPN), within the network itself.\n",
    "\n",
    "The output of this network is combined with the feature map and passed in\n",
    "through a similar pipeline to the Fast R-CNN network, as shown in Figure 7. The\n",
    "Faster R-CNN network is about 10x faster than the Fast R-CNN network, making\n",
    "it approximately 250x faster than an R-CNN network:\n",
    "\n",
    "![](./i/faster_Rnn.png)\n",
    "\n",
    "\n",
    "\n",
    "## You Only Look Once (YOLO)\n",
    "Another somewhat different class of object detection networks are Single Shot\n",
    "Detectors (SSD) such as You Only Look Once (YOLO). In these cases, each image is\n",
    "split into a predefined number of parts using a grid. In the case of YOLO, a 7√ó7 grid\n",
    "is used, resulting in 49 subimages.A predetermined set of crops with different aspect\n",
    "ratios are applied to each sub-image. \n",
    "\n",
    "Given B bounding boxes and C object classes,\n",
    "the output for each image is a vector of size **(7 * 7 * (5B + C))**. Each bounding box has\n",
    "a confidence and coordinates (x, y, w, h), and each grid has prediction probabilities\n",
    "for the different objects detected within them.\n",
    "\n",
    "The YOLO network is a CNN that does this transformation. The final predictions\n",
    "and bounding boxes are found by aggregating the findings from this vector. In\n",
    "YOLO a single convolutional network predicts the bounding boxes and the related\n",
    "class probabilities. YOLO is the faster solution for object detection, but the algorithm\n",
    "might fail to detect smaller objects (an implementation can be found at \n",
    "https://www.kaggle.com/aruchomu/yolo-v3-object-detection-in-tensorflow).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-business",
   "metadata": {},
   "source": [
    "# Instance segmentation\n",
    "\n",
    "Instance segmentation is similar to semantic segmentation ‚Äì the process of\n",
    "associating each pixel of an image with a class label ‚Äì with a few important\n",
    "distinctions. \n",
    "\n",
    "    - First, it needs to distinguish between different instances of the sameclass in an image. \n",
    "    - Second, it is not required to label every single pixel in the image. In some respects, instance segmentation is also similar to object detection, except that instead of bounding boxes, we want to find a binary mask that covers each object.\n",
    "    \n",
    "![](./i/mask_rcnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-audit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
