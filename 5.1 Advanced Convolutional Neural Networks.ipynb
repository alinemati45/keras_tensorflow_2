{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "provincial-prediction",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Advanced-Convolutional-Neural-Networks\" data-toc-modified-id=\"Advanced-Convolutional-Neural-Networks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Advanced Convolutional Neural Networks</a></span></li><li><span><a href=\"#Classification-and-localization\" data-toc-modified-id=\"Classification-and-localization-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Classification and localization</a></span></li><li><span><a href=\"#Semantic-segmentation\" data-toc-modified-id=\"Semantic-segmentation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Semantic segmentation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Approch-1-and-2\" data-toc-modified-id=\"Approch-1-and-2-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Approch 1 and 2</a></span></li><li><span><a href=\"#Approch--CNN-encoder-decoder-network\" data-toc-modified-id=\"Approch--CNN-encoder-decoder-network-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Approch  CNN encoder-decoder network</a></span></li></ul></li><li><span><a href=\"#Object-detection\" data-toc-modified-id=\"Object-detection-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Object detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#classification-and-localization-networks\" data-toc-modified-id=\"classification-and-localization-networks-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>classification and localization networks</a></span></li><li><span><a href=\"#Selective-Search---Region-Proposals-CNN-(R-CNN)\" data-toc-modified-id=\"Selective-Search---Region-Proposals-CNN-(R-CNN)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Selective Search - Region Proposals-CNN (R-CNN)</a></span></li><li><span><a href=\"#SVM-based-classifier\" data-toc-modified-id=\"SVM-based-classifier-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>SVM-based classifier</a></span></li><li><span><a href=\"#Fast-R-CNN\" data-toc-modified-id=\"Fast-R-CNN-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Fast R-CNN</a></span></li><li><span><a href=\"#Faster-R-CNN\" data-toc-modified-id=\"Faster-R-CNN-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Faster R-CNN</a></span></li><li><span><a href=\"#You-Only-Look-Once-(YOLO)\" data-toc-modified-id=\"You-Only-Look-Once-(YOLO)-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>You Only Look Once (YOLO)</a></span></li></ul></li><li><span><a href=\"#Instance-segmentation\" data-toc-modified-id=\"Instance-segmentation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Instance segmentation</a></span></li><li><span><a href=\"#Classifying-Fashion-MNIST-with-a-tf.keras---estimator-model\" data-toc-modified-id=\"Classifying-Fashion-MNIST-with-a-tf.keras---estimator-model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Classifying Fashion-MNIST with a tf.keras - estimator model</a></span></li><li><span><a href=\"#Run-Fashion-MNIST-the-tf.keras---estimator-model-on-GPUs\" data-toc-modified-id=\"Run-Fashion-MNIST-the-tf.keras---estimator-model-on-GPUs-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Run Fashion-MNIST the tf.keras - estimator model on GPUs</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-buffalo",
   "metadata": {},
   "source": [
    "# Advanced Convolutional Neural Networks\n",
    "\n",
    "Different computer vision tasks. Source: Introduction to Artificial Intelligence and Computer\n",
    "Vision Revolution (https://www.slideshare.net/darian_f/introduction-to-the-artificial-intelligence-andcomputer-vision-revolution)\n",
    "\n",
    "![](./i/computer_vision_tasks.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-foundation",
   "metadata": {},
   "source": [
    "# Classification and localization\n",
    "\n",
    "In the classification and localization task not only do you have to report the class of\n",
    "object found in the image, but also the coordinates of the bounding box where the\n",
    "object appears in the image. This type of task assumes that there is only one instance\n",
    "of the object in an image.\n",
    "\n",
    "*This can be achieved by attaching a \"regression head\"  in addition to the\n",
    "\"classification head\" in a typical classification network *\n",
    "\n",
    "    - classification head:   \n",
    "    Recall that in a classification network, the final output of convolution and pooling operations, called the feature map, is fed into a fully connected network that produces a vector of class probabilities. This fully connected network is called the `classification head`, and it is tuned using a categorical loss function (Lc) such as `categorical cross entropy`.\n",
    "\n",
    "    - regression head:\n",
    "    a regression head is another fully connected network that takes the feature map and produces a vector (x, y, w, h) representing the top-left x and y coordinates, width and height of the bounding box. \n",
    "    It is tuned using a continuous loss function (Lr) such as mean squared error. The entire network is tuned using a linear combination of the two losses, that is:\n",
    "$$\n",
    "ùêø = ùõºùêø_C + (1 ‚àí ùõº)ùêø_ùëü\n",
    "$$\n",
    "\n",
    "Here $ùõº$ is a hyperparameter and can take a value between 0 and 1. Unless the value\n",
    "is determined by some domain knowledge about the problem, it can be set to 0.5.\n",
    "\n",
    "![](./i/localizationandclassification_pattern.png)\n",
    "\n",
    "*the only difference with respect to a typical CNN classification network is the additional regression head on the top right*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-labor",
   "metadata": {},
   "source": [
    "# Semantic segmentation\n",
    "the aim is to classify every single pixel on the image as belonging to a single class.\n",
    "\n",
    "## Approch 1 and 2\n",
    "\n",
    "An initial method of implementation could be to build a classifier network for each\n",
    "pixel, where the input is a small neighborhood around each pixel. In practice, this\n",
    "approach is not very performant, so an improvement over this implementation\n",
    "might be to run the image through convolutions that will increase the feature depth,\n",
    "while keeping the image width and height constant. Each pixel then has a feature\n",
    "map that can be sent through a fully connected network that predicts the class of the\n",
    "pixel. However, in practice, this is also quite expensive, and it is not normally used.\n",
    "\n",
    "## Approch  CNN encoder-decoder network\n",
    "the `encoder` decreases the width and height of the image but increases its depth (number of\n",
    "features), while the `decoder` uses transposed convolution operations to increase its\n",
    "size and decrease depth. Transpose convolution (or upsampling) is the process of\n",
    "going in the opposite direction of a normal convolution. The input to this network is\n",
    "the image and the output is the segmentation map.\n",
    "\n",
    "A popular implementation of this encoder-decoder architecture is the U-Net. (https://github.com/jakeret/tf_unet)\n",
    "\n",
    "U-Net: Convolutional Networks for Biomedical Image Segmentation:\n",
    "![](./i/u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-warning",
   "metadata": {},
   "source": [
    "# Object detection\n",
    "The object detection task is similar to the classification and localization tasks. The big\n",
    "difference is that now there are multiple objects in the image, and for each one we\n",
    "need to find the class and bounding box coordinates. In addition, neither the number\n",
    "of objects nor their size is known in advance. As you can imagine, this is a difficult\n",
    "problem and a fair amount of research has gone into it.\n",
    "\n",
    "\n",
    "##  classification and localization networks\n",
    "A first approach to the problem might be to create many random crops of the\n",
    "input image and for each crop, apply the classification and localization networks\n",
    "we described earlier. However, such an approach is very wasteful in terms of\n",
    "computing and unlikely to be very successful.\n",
    "\n",
    "## Selective Search - Region Proposals-CNN (R-CNN)\n",
    "A more practical approach would be use a tool such as Selective Search (Selective\n",
    "Search for Object Recognition, by Uijlings et al.\n",
    "(http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)\n",
    "\n",
    "uses traditional computer\n",
    "vision techniques to find areas in the image that might contain objects. These regions\n",
    "are called \"Region Proposals,\" and the network to detect them was called \"Region\n",
    "Proposal Network,\" or R-CNN. In the original R-CNN, the regions were resized\n",
    "and fed into a network to yield image vectors:\n",
    "\n",
    "![](./i/R_cnn_cnn.png)\n",
    "\n",
    "## SVM-based classifier\n",
    "These vectors were then classified with an `SVM-based classifier` (https://\n",
    "en.wikipedia.org/wiki/Support-vector_machine), and the bounding boxes\n",
    "proposed by the external tool were corrected using a linear regression network over\n",
    "the image vectors. A R-CNN network can be represented conceptually as shown in\n",
    "Figure 5:\n",
    "\n",
    "![](./i/R_cnn_Network.png)\n",
    "\n",
    "## Fast R-CNN\n",
    "The next iteration of the R-CNN network was called the `Fast R-CNN`. The Fast\n",
    "R-CNN still gets its region proposals from an external tool, but instead of feeding\n",
    "each region proposal through the CNN, the entire image is fed through the CNN\n",
    "and the region proposals are projected onto the resulting feature map. Each region of\n",
    "interest is fed through an **Region of Interest (ROI)** pooling layer and then to a fully\n",
    "connected network, which produces a feature vector for the ROI. \n",
    "\n",
    "ROI pooling is a widely used operation in object detection tasks using convolutional\n",
    "neural networks. The ROI pooling layer uses max pooling to convert the features\n",
    "inside any valid region of interest into a small feature map with a fixed spatial extent\n",
    "of H √ó W (where H and W are two hyperparameters). The feature vector is then\n",
    "fed into two fully connected networks, one to predict the class of the ROI and the\n",
    "other to correct the bounding box coordinates for the proposal. This is illustrated\n",
    "in Figure 6:\n",
    "\n",
    "![](./i/fast_Rnn.png)\n",
    "\n",
    "## Faster R-CNN\n",
    "The Fast R-CNN is about 25x faster than the R-CNN. The next improvement, called\n",
    "the Faster R-CNN (an implementation can be found at https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN), removes the\n",
    "external region proposal mechanism and replaces it with a trainable component,\n",
    "called the Region Proposal Network (RPN), within the network itself.\n",
    "\n",
    "The output of this network is combined with the feature map and passed in\n",
    "through a similar pipeline to the Fast R-CNN network, as shown in Figure 7. The\n",
    "Faster R-CNN network is about 10x faster than the Fast R-CNN network, making\n",
    "it approximately 250x faster than an R-CNN network:\n",
    "\n",
    "![](./i/faster_Rnn.png)\n",
    "\n",
    "\n",
    "\n",
    "## You Only Look Once (YOLO)\n",
    "Another somewhat different class of object detection networks are Single Shot\n",
    "Detectors (SSD) such as You Only Look Once (YOLO). In these cases, each image is\n",
    "split into a predefined number of parts using a grid. In the case of YOLO, a 7√ó7 grid\n",
    "is used, resulting in 49 subimages.A predetermined set of crops with different aspect\n",
    "ratios are applied to each sub-image. \n",
    "\n",
    "Given B bounding boxes and C object classes,\n",
    "the output for each image is a vector of size **(7 * 7 * (5B + C))**. Each bounding box has\n",
    "a confidence and coordinates (x, y, w, h), and each grid has prediction probabilities\n",
    "for the different objects detected within them.\n",
    "\n",
    "The YOLO network is a CNN that does this transformation. The final predictions\n",
    "and bounding boxes are found by aggregating the findings from this vector. In\n",
    "YOLO a single convolutional network predicts the bounding boxes and the related\n",
    "class probabilities. YOLO is the faster solution for object detection, but the algorithm\n",
    "might fail to detect smaller objects (an implementation can be found at \n",
    "https://www.kaggle.com/aruchomu/yolo-v3-object-detection-in-tensorflow).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-business",
   "metadata": {},
   "source": [
    "# Instance segmentation\n",
    "\n",
    "Instance segmentation is similar to semantic segmentation ‚Äì the process of\n",
    "associating each pixel of an image with a class label ‚Äì with a few important\n",
    "distinctions. \n",
    "\n",
    "    - First, it needs to distinguish between different instances of the sameclass in an image. \n",
    "    - Second, it is not required to label every single pixel in the image. In some respects, instance segmentation is also similar to object detection, except that instead of bounding boxes, we want to find a binary mask that covers each object.\n",
    "    \n",
    "![](./i/mask_rcnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-albany",
   "metadata": {},
   "source": [
    "# Classifying Fashion-MNIST with a tf.keras - estimator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confirmed-diploma",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T03:49:48.390859Z",
     "start_time": "2021-10-08T03:49:48.370856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.test.gpu_device_name()\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "backed-kazakhstan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T03:49:53.715672Z",
     "start_time": "2021-10-08T03:49:53.282571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# How many categories we are predicting from (0-9)\n",
    "LABEL_DIMENSIONS = 10\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "TRAINING_SIZE = len(train_images)\n",
    "TEST_SIZE = len(test_images)\n",
    "\n",
    "train_images = np.asarray(train_images, dtype=np.float32) / 255\n",
    "\n",
    "# Convert the train images and add channels\n",
    "train_images = train_images.reshape((TRAINING_SIZE, 28, 28, 1))\n",
    "\n",
    "test_images = np.asarray(test_images, dtype=np.float32) / 255\n",
    "# Convert the train images and add channels\n",
    "test_images = test_images.reshape((TEST_SIZE, 28, 28, 1))\n",
    "\n",
    "train_labels  = tf.keras.utils.to_categorical(train_labels, LABEL_DIMENSIONS)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, LABEL_DIMENSIONS)\n",
    "\n",
    "# Cast the labels to float\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)\n",
    "print (train_labels.shape)\n",
    "print (test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-quebec",
   "metadata": {},
   "source": [
    "**build** a CNN model - using the functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "played-concern",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T03:50:56.977191Z",
     "start_time": "2021-10-08T03:50:56.617874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 256)         147712    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 323,658\n",
      "Trainable params: 323,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(28,28,1))  \n",
    "x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "x = tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "predictions = tf.keras.layers.Dense(LABEL_DIMENSIONS, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "occupational-empty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T03:54:01.846423Z",
     "start_time": "2021-10-08T03:54:01.826424Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aggregate-illustration",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T03:57:12.214704Z",
     "start_time": "2021-10-08T03:57:12.163349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "#strategy = None\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "config = tf.estimator.RunConfig(train_distribute=strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "legendary-intake",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T03:57:21.512074Z",
     "start_time": "2021-10-08T03:57:21.042042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\nemat\\AppData\\Local\\Temp\\tmpc3yzkk64\n",
      "INFO:tensorflow:Using the Keras model provided.\n",
      "WARNING:tensorflow:From C:\\Users\\nemat\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\nemat\\\\AppData\\\\Local\\\\Temp\\\\tmpc3yzkk64', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x00000275E47C9A48>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000275E47C9588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.keras.estimator.model_to_estimator(model, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "generous-subscriber",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T03:59:03.611473Z",
     "start_time": "2021-10-08T03:57:35.362354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='C:\\\\Users\\\\nemat\\\\AppData\\\\Local\\\\Temp\\\\tmpc3yzkk64\\\\keras\\\\keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: C:\\Users\\nemat\\AppData\\Local\\Temp\\tmpc3yzkk64\\keras\\keras_model.ckpt\n",
      "INFO:tensorflow:Warm-starting variables only in TRAINABLE_VARIABLES.\n",
      "INFO:tensorflow:Warm-started 12 variables.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\nemat\\AppData\\Local\\Temp\\tmpc3yzkk64\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3017294, step = 0\n",
      "INFO:tensorflow:global_step/sec: 107.688\n",
      "INFO:tensorflow:loss = 2.278116, step = 100 (0.929 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.47\n",
      "INFO:tensorflow:loss = 2.2200718, step = 200 (0.697 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.905\n",
      "INFO:tensorflow:loss = 1.8720225, step = 300 (0.695 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.93\n",
      "INFO:tensorflow:loss = 1.2353915, step = 400 (0.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.768\n",
      "INFO:tensorflow:loss = 1.1601584, step = 500 (0.715 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.869\n",
      "INFO:tensorflow:loss = 1.033916, step = 600 (0.750 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.71\n",
      "INFO:tensorflow:loss = 0.9636233, step = 700 (0.714 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.423\n",
      "INFO:tensorflow:loss = 0.90580755, step = 800 (0.691 sec)\n",
      "INFO:tensorflow:global_step/sec: 134.903\n",
      "INFO:tensorflow:loss = 0.95719206, step = 900 (0.741 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.936\n",
      "INFO:tensorflow:loss = 0.86493456, step = 1000 (0.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.388\n",
      "INFO:tensorflow:loss = 0.8287459, step = 1100 (0.697 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.329\n",
      "INFO:tensorflow:loss = 0.89569014, step = 1200 (0.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.914\n",
      "INFO:tensorflow:loss = 0.8425482, step = 1300 (0.706 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.461\n",
      "INFO:tensorflow:loss = 0.80763936, step = 1400 (0.711 sec)\n",
      "INFO:tensorflow:global_step/sec: 136.057\n",
      "INFO:tensorflow:loss = 0.71135366, step = 1500 (0.736 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.326\n",
      "INFO:tensorflow:loss = 0.6810385, step = 1600 (0.712 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.845\n",
      "INFO:tensorflow:loss = 0.7851701, step = 1700 (0.711 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.562\n",
      "INFO:tensorflow:loss = 0.76085424, step = 1800 (0.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.575\n",
      "INFO:tensorflow:loss = 0.74748486, step = 1900 (0.692 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.585\n",
      "INFO:tensorflow:loss = 0.70522535, step = 2000 (0.727 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.353\n",
      "INFO:tensorflow:loss = 0.6417326, step = 2100 (0.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.211\n",
      "INFO:tensorflow:loss = 0.6432152, step = 2200 (0.693 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.945\n",
      "INFO:tensorflow:loss = 0.7110345, step = 2300 (0.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.331\n",
      "INFO:tensorflow:loss = 0.62365294, step = 2400 (0.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.591\n",
      "INFO:tensorflow:loss = 0.692796, step = 2500 (0.692 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.779\n",
      "INFO:tensorflow:loss = 0.61383885, step = 2600 (0.709 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.111\n",
      "INFO:tensorflow:loss = 0.6204662, step = 2700 (0.699 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.883\n",
      "INFO:tensorflow:loss = 0.5710565, step = 2800 (0.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.756\n",
      "INFO:tensorflow:loss = 0.52717376, step = 2900 (0.715 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.206\n",
      "INFO:tensorflow:loss = 0.5989653, step = 3000 (0.729 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.857\n",
      "INFO:tensorflow:loss = 0.584213, step = 3100 (0.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.576\n",
      "INFO:tensorflow:loss = 0.5635039, step = 3200 (0.687 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.242\n",
      "INFO:tensorflow:loss = 0.57733816, step = 3300 (0.688 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.385\n",
      "INFO:tensorflow:loss = 0.57984155, step = 3400 (0.702 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.835\n",
      "INFO:tensorflow:loss = 0.5470796, step = 3500 (0.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.655\n",
      "INFO:tensorflow:loss = 0.55617815, step = 3600 (0.687 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.142\n",
      "INFO:tensorflow:loss = 0.582893, step = 3700 (0.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.519\n",
      "INFO:tensorflow:loss = 0.5476088, step = 3800 (0.698 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.744\n",
      "INFO:tensorflow:loss = 0.65033406, step = 3900 (0.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.329\n",
      "INFO:tensorflow:loss = 0.46885422, step = 4000 (0.712 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.119\n",
      "INFO:tensorflow:loss = 0.5730246, step = 4100 (0.720 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.344\n",
      "INFO:tensorflow:loss = 0.5515729, step = 4200 (0.692 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.341\n",
      "INFO:tensorflow:loss = 0.43049172, step = 4300 (0.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.043\n",
      "INFO:tensorflow:loss = 0.58201516, step = 4400 (0.715 sec)\n",
      "INFO:tensorflow:global_step/sec: 146.83\n",
      "INFO:tensorflow:loss = 0.5117538, step = 4500 (0.680 sec)\n",
      "INFO:tensorflow:global_step/sec: 146.258\n",
      "INFO:tensorflow:loss = 0.47957134, step = 4600 (0.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 147.647\n",
      "INFO:tensorflow:loss = 0.5285442, step = 4700 (0.677 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.348\n",
      "INFO:tensorflow:loss = 0.48062626, step = 4800 (0.688 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.846\n",
      "INFO:tensorflow:loss = 0.49774438, step = 4900 (0.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.94\n",
      "INFO:tensorflow:loss = 0.47609538, step = 5000 (0.695 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.146\n",
      "INFO:tensorflow:loss = 0.488823, step = 5100 (0.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 146.115\n",
      "INFO:tensorflow:loss = 0.5829551, step = 5200 (0.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.305\n",
      "INFO:tensorflow:loss = 0.48164022, step = 5300 (0.688 sec)\n",
      "INFO:tensorflow:global_step/sec: 146.024\n",
      "INFO:tensorflow:loss = 0.5522422, step = 5400 (0.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.061\n",
      "INFO:tensorflow:loss = 0.45473975, step = 5500 (0.694 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.622\n",
      "INFO:tensorflow:loss = 0.5041953, step = 5600 (0.696 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.906\n",
      "INFO:tensorflow:loss = 0.43467173, step = 5700 (0.695 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.741\n",
      "INFO:tensorflow:loss = 0.46070516, step = 5800 (0.687 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5860 into C:\\Users\\nemat\\AppData\\Local\\Temp\\tmpc3yzkk64\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.54697675.\n",
      "<tensorflow_estimator.python.estimator.estimator.EstimatorV2 object at 0x00000275E4AD6788>\n"
     ]
    }
   ],
   "source": [
    "def input_fn(images, labels, epochs, batch_size):\n",
    "    # Convert the inputs to a Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples. \n",
    "    SHUFFLE_SIZE = 5000\n",
    "    dataset = dataset.shuffle(SHUFFLE_SIZE).repeat(epochs).batch(batch_size)\n",
    "    dataset = dataset.prefetch(None)\n",
    "\n",
    "    # Return the dataset. \n",
    "    return dataset\n",
    "\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 50\n",
    "\n",
    "#time_hist = TimeHistory()\n",
    "\n",
    "estimator_train_result = estimator.train(input_fn=lambda:input_fn(train_images,\n",
    "                                         train_labels,\n",
    "                                         epochs=EPOCHS,\n",
    "                                         batch_size=BATCH_SIZE))\n",
    "print(estimator_train_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-collapse",
   "metadata": {},
   "source": [
    "with 2.0alpha If i use strategy = tf.distribute.MirroredStrategy() and GPUS then I get an error \"ValueError: Only TensorFlow native optimizers are supported with DistributionStrategy.\" Reported the problem to https://github.com/tensorflow/tensorflow/issues/27696 fixed in night-build.\n",
    "\n",
    "This seems fixed in TF2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "large-turning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T03:59:07.981075Z",
     "start_time": "2021-10-08T03:59:05.923201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-10-07T22:59:06Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\nemat\\AppData\\Local\\Temp\\tmpc3yzkk64\\model.ckpt-5860\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2021-10-07-22:59:06\n",
      "INFO:tensorflow:Saving dict for global step 5860: accuracy = 0.8244, global_step = 5860, loss = 0.48151103\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5860: C:\\Users\\nemat\\AppData\\Local\\Temp\\tmpc3yzkk64\\model.ckpt-5860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8244, 'loss': 0.48151103, 'global_step': 5860}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(lambda:input_fn(test_images, \n",
    "                                   test_labels,\n",
    "                                   epochs=1,\n",
    "                                   batch_size=BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-sweden",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-bacon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
