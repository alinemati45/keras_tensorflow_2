{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "durable-honey",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Video\" data-toc-modified-id=\"Video-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Video</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classifying-videos-with-pretrained-nets-in-six-different-ways\" data-toc-modified-id=\"Classifying-videos-with-pretrained-nets-in-six-different-ways-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Classifying videos with pretrained nets in six different ways</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-contact",
   "metadata": {},
   "source": [
    "# Video\n",
    "## Classifying videos with pretrained nets in six different ways\n",
    "\n",
    "The **first approach** consists of classifying one video frame at a time by considering\n",
    "each one of them as a separate image processed with a 2D CNN. This approach\n",
    "simply reduces the video classification problem to an image classification problem.\n",
    "Each video frame \"emits\" a classification output, and the video is classified by taking\n",
    "into account the more frequently chosen category for each frame.\n",
    "\n",
    "The **second approach** consists of creating one single network where a 2D CNN is\n",
    "combined with an RNN (see Chapter 9, Autoencoders). The idea is that the CNN will\n",
    "take into account the image components and the RNN will take into account the\n",
    "sequence information for each video. This type of network can be very difficult to\n",
    "train because of the very high number of parameters to optimize.\n",
    "\n",
    "The **third approach** is to use a 3D ConvNet, where 3D ConvNets are an extension\n",
    "of 2D ConvNets operating on a 3D tensor (time, image_width, image_height). This\n",
    "approach is another natural extension of image classification. Again, 3D ConvNets\n",
    "can be hard to train.\n",
    "\n",
    "The **fourth approach** is based on a clever idea: instead of using CNNs directly for\n",
    "classification, they can be used for storing offline features for each frame in the video.\n",
    "The idea is that feature extraction can be made very efficient with transfer learning\n",
    "as shown in a previous chapter. After all features are extracted, they can be passed\n",
    "as a set of inputs into an RNN, which will learn sequences across multiple frames\n",
    "and emit the final classification.\n",
    "\n",
    "The **fifth approach** is a simple variant of the fourth, where the final layer is an MLP\n",
    "instead of an RNN. In certain situations, this approach can be simpler and less\n",
    "expensive in terms of computational requirements.\n",
    "\n",
    "The **sixth approach** is a variant of the fourth, where the phase of feature extraction is\n",
    "realized with a 3D CNN that extracts spatial and visual features. These features are\n",
    "then passed into either an RNN or an MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "touched-alabama",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:07:04.452477Z",
     "start_time": "2021-11-08T19:07:04.443478Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, preprocessing\n",
    "import tensorflow_datasets as tfds\n",
    "max_len = 200\n",
    "n_words = 10000\n",
    "dim_embedding = 256\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE =500\n",
    "def load_data():\n",
    "    #load data\n",
    "    (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_words)\n",
    "    X_train = preprocessing.sequence.pad_sequences(X_train,maxlen=max_len)\n",
    "    X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "intermediate-relay",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:07:58.291219Z",
     "start_time": "2021-11-08T19:07:54.818142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 256)          2560000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 198, 256)          196864    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,789,889\n",
      "Trainable params: 2,789,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    # Input - Embedding Layer\n",
    "    # the model will take as input an integer matrix of size\n",
    "    # (batch, input_length)\n",
    "    # the model will output dimension (input_length, dim_embedding)\n",
    "    # the largest integer in the input should be no larger\n",
    "    # than n_words (vocabulary size).\n",
    "    model.add(layers.Embedding(n_words,\n",
    "    dim_embedding, input_length=max_len))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv1D(256, 3, padding='valid',\n",
    "    activation='relu'))\n",
    "    # takes the maximum value of either feature vector from each of\n",
    "    # the n_words features\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "model=build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "little-cooperative",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:10:02.733811Z",
     "start_time": "2021-11-08T19:08:19.555434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 9s 372us/sample - loss: 0.6580 - accuracy: 0.6190 - val_loss: 0.5401 - val_accuracy: 0.7471\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 5s 197us/sample - loss: 0.4118 - accuracy: 0.8186 - val_loss: 0.3125 - val_accuracy: 0.8672\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 5s 197us/sample - loss: 0.2566 - accuracy: 0.8963 - val_loss: 0.2713 - val_accuracy: 0.8845\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 5s 197us/sample - loss: 0.1707 - accuracy: 0.9401 - val_loss: 0.2772 - val_accuracy: 0.8862\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 5s 197us/sample - loss: 0.1043 - accuracy: 0.9667 - val_loss: 0.2882 - val_accuracy: 0.8864\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 5s 197us/sample - loss: 0.0559 - accuracy: 0.9859 - val_loss: 0.3238 - val_accuracy: 0.8846\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 5s 198us/sample - loss: 0.0276 - accuracy: 0.9944 - val_loss: 0.3626 - val_accuracy: 0.8824\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 5s 200us/sample - loss: 0.0154 - accuracy: 0.9977 - val_loss: 0.3968 - val_accuracy: 0.8834\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 5s 198us/sample - loss: 0.0091 - accuracy: 0.9992 - val_loss: 0.4369 - val_accuracy: 0.8785\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 5s 199us/sample - loss: 0.0062 - accuracy: 0.9993 - val_loss: 0.4663 - val_accuracy: 0.8783\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 5s 202us/sample - loss: 0.0043 - accuracy: 0.9994 - val_loss: 0.4843 - val_accuracy: 0.8792\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 5s 200us/sample - loss: 0.0030 - accuracy: 0.9998 - val_loss: 0.5061 - val_accuracy: 0.8794\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 5s 195us/sample - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.5315 - val_accuracy: 0.8767\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 5s 200us/sample - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.5437 - val_accuracy: 0.8792\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 5s 198us/sample - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.5598 - val_accuracy: 0.8783\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 5s 197us/sample - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.5679 - val_accuracy: 0.8794\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 5s 196us/sample - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.5856 - val_accuracy: 0.8790\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 5s 194us/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.6075 - val_accuracy: 0.8761\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 5s 194us/sample - loss: 8.5303e-04 - accuracy: 1.0000 - val_loss: 0.6180 - val_accuracy: 0.8778\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 5s 194us/sample - loss: 9.0467e-04 - accuracy: 1.0000 - val_loss: 0.6262 - val_accuracy: 0.8760\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"] )\n",
    "\n",
    "\n",
    "score = model.fit(X_train, y_train,\n",
    "                epochs= EPOCHS,\n",
    "                batch_size = BATCH_SIZE,\n",
    "                validation_data = (X_test, y_test)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pleased-details",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T19:17:37.473840Z",
     "start_time": "2021-11-08T19:17:35.968552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test score: 0.6261743384599686\n",
      "Test accuracy: 0.87596\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE , verbose = 0)\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-administration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
